[{"authors":[],"categories":["数据科学项目","特征衍生","特征工程"],"content":" Table of Contents 1\u0026nbsp;\u0026nbsp;衍生特征分析过程1.1\u0026nbsp;\u0026nbsp;数据读取1.2\u0026nbsp;\u0026nbsp;定义处理单列的数据1.2.1\u0026nbsp;\u0026nbsp;单列数据的观察1.2.1.1\u0026nbsp;\u0026nbsp;家庭结构有关1.2.1.2\u0026nbsp;\u0026nbsp;社交账号有关1.2.1.3\u0026nbsp;\u0026nbsp;购物数据（京东账号jd_account、淘宝账号alipay_account）1.2.1.4\u0026nbsp;\u0026nbsp;有关收入（月收入month_income、家庭支出support_pre_pay）1.2.2\u0026nbsp;\u0026nbsp;处理的基本函数1.2.3\u0026nbsp;\u0026nbsp;目标变量处理（逾期数cd）1.2.4\u0026nbsp;\u0026nbsp;组合-微信、微博、qq账户数量：社交数量shejiao_num1.2.5\u0026nbsp;\u0026nbsp;组合-京东账号、淘宝账号：购物账号数量gouwu_num1.2.6\u0026nbsp;\u0026nbsp;组合-marriage、is_child、child_sum、support_pre_count：家庭结构hunyin_cata1.2.7\u0026nbsp;\u0026nbsp;单列-月收入数据：收入分级monthincome_bins1.2.8\u0026nbsp;\u0026nbsp;组合-月收入、家庭支出：家庭支出比重zhichu_ratio1.2.8.1\u0026nbsp;\u0026nbsp;zhichu_ratio生成和分布情况观察1.2.8.2\u0026nbsp;\u0026nbsp;尝试（log变换）1.2.8.3\u0026nbsp;\u0026nbsp;zhichu_ratio的衍生特征——zhichu_ratio_addcata1.3\u0026nbsp;\u0026nbsp;变量重要性判别1.3.1\u0026nbsp;\u0026nbsp;从统计相关的角度分析单变量的影响1.3.1.1\u0026nbsp;\u0026nbsp;社交和购物数据视为连续—离散1.3.1.2\u0026nbsp;\u0026nbsp;社交和购物数据视为离散—离散1.3.1.3\u0026nbsp;\u0026nbsp;家庭支出比重（zhichu_ratio）的连续—离散1.3.2\u0026nbsp;\u0026nbsp;从多变量结合角度1.3.2.1\u0026nbsp;\u0026nbsp;离散—离散1.3.2.2\u0026nbsp;\u0026nbsp;采用高级统计方法识别特征效用1.3.3\u0026nbsp;\u0026nbsp;结论\n衍生特征分析过程 import pandas as pd import numpy as np import datetime import xgboost as xgb import re import seaborn as sns import matplotlib.pyplot as plt %matplotlib notebook plt.rcParams['font.sans-serif']=['SimHei'] # show Chinese in chart pd.set_option('display.max_columns', 40) # 显示隐藏列  数据读取 path2 = r'/home/hadoop/jack_xxx/xianxia_client_2.csv' dataset2 = pd.read_csv(path2, sep=',', encoding='utf-8', index_col=[0])  C:\\Users\\M4500\\Anaconda3\\envs\\py35_xgboost\\lib\\site-packages\\numpy\\lib\\arraysetops.py:472: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison mask |= (ar1 == a)  dataset2.drop_duplicates(subset=['intopieces_id'], inplace=True)  dataset2.tail()  定义处理单列的数据 单列数据的观察 家庭结构有关 child_sum  child_sum数据出现的值, [ 0., 2., 1., 3., 4., nan, 5., 8., 62., 6., 7., 11., 68., 100., 22., 15., 12., 25., 113., 20., 10., 41., 33., 14., 9., 21., 24., 23.],需预先对该列处理\n dataset2.groupby('child_sum').count()[['intopieces_id']].T   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }   child_sum 0.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 10.0 11.0 12.0 14.0 15.0 20.0 21.0 22.0 23.0 24.0 25.0 33.0 41.0 62.0 68.0 100.0 113.0     intopieces_id 323802 633416 304946 40737 5433 789 143 20 13 1 6 41 5 2 1 8 2 11 1 1 2 1 1 10 1 5 1     fig1,ax1 = plt.subplots(1,1) ax1.get_xaxis().set_visible(False) # Hide Ticks # pd.plotting.table(ax1, dataset2.groupby('child_sum').count()[['intopieces_id']], loc='upper right',colWidths=[0.2,0.2,0.2]) pl11 = dataset2.groupby(['child_sum']).count()['intopieces_id'].plot(kind='bar', title='child_sum',table=True, ax=ax1,figsize=(10,10),fontsize=12)  将child_sum\u0026gt;=5的部分视为异常数据，暂且先保留，在处理当中视为其它。\nmarriage 进件信息表中该字段的说明，婚姻状态0未婚1已婚2离异3丧偶。\ndataset2.groupby(['marriage']).count()['intopieces_id']  marriage 0 65785 1 211796 2 937405 3 143062 4 7971 5 8866 Name: intopieces_id, dtype: int64  fig2,ax2 = plt.subplots(1,1) ax2.get_xaxis().set_visible(False) # Hide Ticks dataset2.groupby(['marriage']).count()['intopieces_id'].plot(kind='bar', title='婚姻状态',table=True, ax=ax2,figsize=(10,10),fontsize=12)  \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0xa6b6518\u0026gt;  对该字段，marriage\u0026gt;=4的部分视为异常，暂且保留，归为其它类别。\nis_child 进件信息表对该字段说明，有无子女/1.有2.无。\ndataset2.groupby(['is_child']).count()['intopieces_id']  is_child 0 1372175 1 2071 2 639 Name: intopieces_id, dtype: int64  # fig1 = plt.figure() # fig1.add_subplot(111) fig3,ax3 = plt.subplots(1,1) ax3.get_xaxis().set_visible(False) # Hide Ticks pl2 = dataset2.groupby(['is_child']).count()['intopieces_id'].plot(kind='bar', title='子女（有无）',table=True, ax=ax3, figsize=(8,8))  对该字段的观察，字段说明可能有问题，在这里理解应为0-有，1-无，2-其它。故这里的处理，将is_child\u0026gt;1视为其它。\nsupport_pre_count dataset2.groupby(['support_pre_count']).count()['intopieces_id']  support_pre_count 0 814769 1 398249 2 144316 3 15426 4 1766 5 253 6 43 7 4 9 1 10 3 11 10 12 1 14 2 15 1 16 1 20 3 21 1 22 3 25 1 36 2 41 1 127 29 Name: intopieces_id, dtype: int64  fig4,ax4 = plt.subplots(1,1) ax4.get_xaxis().set_visible(False) # Hide Ticks dataset2.groupby(['support_pre_count']).count()['intopieces_id'].plot(kind='bar', title='供养人数',table=True, ax=ax4, figsize=(10,10),fontsize=12)  \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0xa73e2e8\u0026gt;  这里看到供养人数有出现＞6的情况。这儿采用的处理方式，对support_pre_count＞6的部分视为异常，归为其它类别。\n处理方法结论 筛选该条件(support_pre_count \u0026lt;=6) \u0026amp; (is_child\u0026lt;2) \u0026amp; (marriage\u0026lt;4) \u0026amp; (child_sum\u0026lt;5)，在此基础上做家庭结构的划分；对于该条件之外的视为其它类别（归因于数据记录不规范、错误所带来的）\n社交账号有关 dataset2.columns  Index(['intopieces_id', 'cd', 'weixin', 'weibo', 'qq', 'marriage', 'is_child', 'support_pre_count', 'child_sum', 'alipay_account', 'jd_account', 'id_num', 'house_pay', 'support_pre_pay', 'month_income'], dtype='object')  weixin print('空值:',dataset2[dataset2['weixin'].isnull()].count()['intopieces_id'],'\\n', '非空:',dataset2[dataset2['weixin'].notnull()].count()['intopieces_id'],'\\n', '无:',dataset2[(dataset2['weixin']=='无')].count()['intopieces_id']) # [Nn][Aa][Nn]  空值: 1181935 非空: 192950 无: 2434  dataset2[((dataset2['weixin']=='nan')|(dataset2['weixin']=='NULL')|(dataset2['weixin']=='空')|(dataset2['weixin']=='无'))].count()['intopieces_id']  2434  weibo print('空值:',dataset2[dataset2['weibo'].isnull()].count()['intopieces_id'],'\\n', '非空:',dataset2[dataset2['weibo'].notnull()].count()['intopieces_id'],'\\n', '无:',dataset2[(dataset2['weibo']==u'无')].count()['intopieces_id']) # [Nn][Aa][Nn]  空值: 1369308 非空: 5577 无: 3908  dataset2[((dataset2['weibo']=='nan')|(dataset2['weibo']=='NULL')|(dataset2['weibo']=='空')|(dataset2['weibo']=='无'))].count()['intopieces_id']  3908  qq print('空值:',dataset2[dataset2['qq'].isnull()].count()['intopieces_id'],'\\n', '非空:',dataset2[dataset2['qq'].notnull()].count()['intopieces_id'],'\\n', '无:',dataset2[(dataset2['qq']=='无')].count()['intopieces_id']) # [Nn][Aa][Nn]  空值: 1284821 非空: 90064 无: 2611  dataset2[((dataset2['qq']=='nan')|(dataset2['qq']=='NULL')|(dataset2['qq']=='空')|(dataset2['qq']=='无'))].count()['intopieces_id']  2611  处理方式 考虑到三个字段的缺失值比例较大，有几种处理方式： 1、保存原有的缺失(不推荐) 2、插补：选择多重插补法(最合适)\n购物数据（京东账号jd_account、淘宝账号alipay_account） jd_account print('空值:',dataset2[dataset2['jd_account'].isnull()].count()['intopieces_id'],'\\n', '非空:',dataset2[dataset2['jd_account'].notnull()].count()['intopieces_id'],'\\n', '无:',dataset2[(dataset2['jd_account']=='无')].count()['intopieces_id'])  空值: 1371477 非空: 3408 无: 1775  dataset2[((dataset2['jd_account']=='nan')|(dataset2['jd_account']=='NULL')|(dataset2['jd_account']=='空')|(dataset2['jd_account']=='无'))].count()['intopieces_id']  1775  alipay_account dataset2.columns  Index(['intopieces_id', 'cd', 'weixin', 'weibo', 'qq', 'marriage', 'is_child', 'support_pre_count', 'child_sum', 'alipay_account', 'jd_account', 'id_num', 'house_pay', 'support_pre_pay', 'month_income'], dtype='object')  print('空值:',dataset2[dataset2['alipay_account'].isnull()].count()['intopieces_id'],'\\n', '非空:',dataset2[dataset2['alipay_account'].notnull()].count()['intopieces_id'],'\\n', '无:',dataset2[(dataset2['alipay_account']=='无')].count()['intopieces_id']) # [Nn][Aa][Nn]  空值: 1363912 非空: 10973 无: 1369  dataset2[((dataset2['alipay_account']=='nan')|(dataset2['alipay_account']=='NULL')|(dataset2['alipay_account']=='空')|(dataset2['alipay_account']=='无'))].count()['intopieces_id']  1369  有关收入（月收入month_income、家庭支出support_pre_pay） 月收入month_income print('空值:',dataset2[dataset2['month_income'].isnull()].count()['intopieces_id'],'\\n', '非空:',dataset2[dataset2['month_income'].notnull()].count()['intopieces_id'],'\\n', # '无:',dataset2[(dataset2['month_income']=='无')].count()['intopieces_id'] ) # [Nn][Aa][Nn]  空值: 0 非空: 1374885  dataset2[['month_income']].describe()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    month_income     count 1.374885e+06   mean 3.247747e+04   std 1.201198e+06   min 0.000000e+00   25% 4.500000e+03   50% 8.000000e+03   75% 2.000000e+04   max 1.111111e+09     fig41 = plt.figure() fig41.add_subplot(111) dataset2[dataset2['month_income']\u0026gt;= 5000000].groupby('cd').count()['intopieces_id'].plot(kind='bar',title='month_income')  \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0xa80cc50\u0026gt;  fig41 = plt.figure() fig41.add_subplot(111) dataset2[dataset2['month_income']\u0026lt;10000000].groupby('cd').count()['intopieces_id'].plot(kind='bar')  \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0xaef1cc0\u0026gt;  fig41 = plt.figure() fig41.add_subplot(111) plot1 = sns.boxplot(y='month_income',x='cd',data=dataset2, ) plot1.set(ylim=(0,100000)) plt.title('家庭月收入分布')  Text(0.5,1,'家庭月收入分布')  家庭支出support_pre_pay print('空值:',dataset2[dataset2['support_pre_pay'].isnull()].count()['intopieces_id'],'\\n', '非空:',dataset2[dataset2['support_pre_pay'].notnull()].count()['intopieces_id'],'\\n', # '无:',dataset2[(dataset2['month_income']=='无')].count()['intopieces_id'] ) # [Nn][Aa][Nn]  空值: 0 非空: 1374885  dataset2[['support_pre_pay']].describe()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    support_pre_pay     count 1.374885e+06   mean 5.408445e+03   std 3.172173e+06   min 0.000000e+00   25% 0.000000e+00   50% 6.000000e+02   75% 1.000000e+03   max 2.147484e+09     fig42 = plt.figure() fig42.add_subplot(111) plot1 = sns.boxplot(y='support_pre_pay',x='cd',data=dataset2) plot1.set(ylim=(0,10000)) plt.title('家庭月支出分布')  Text(0.5,1,'家庭月支出分布')  处理的基本函数 dataset2[dataset2['child_sum'].isnull()].head()  def one_feature(x): ''' 该函数用于处理单变量特征：社交账号、购物账号 Args: x: string, 账号信息 Returns:number, 打标签[0,1,10] ''' x = str(x) r = '\\s|\\.|\\,|\\:|\\\u0026quot;|\\'|' # '.', ',',' ' x = re.sub(r, \u0026quot;\u0026quot;, x) if x == u'无': label = 0 # 0表示没有账号 elif x == 'nan': label = 10 # 10表示空值标示 else: label = 1 # 1标示有账号 return label def shejiao_func(x, y, z): ''' 组合-社交账户数据的处理 Args: x: string, 微信账号 y: string, 微博账号 z: string, qq账号 Returns: string, 打标签-社交账号类别[0,1,2,3,4,5,6,7,8,9] ''' label_wx = one_feature(x) label_wb = one_feature(y) label_qq = one_feature(z) sum_num = label_wb + label_wx + label_qq if (sum_num \u0026lt;= 3) and (sum_num \u0026gt;= 0): label = str(sum_num) # 正常计数 elif sum_num == 11: label = str(4) # 计数1+未登记1 elif sum_num == 21: label = str(5) # 计数1+未登记2 elif sum_num == 12: label = str(6) # 计数2+未登记1 elif sum_num == 10: label = str(7) # 计数0+未登记1 elif sum_num == 20: label = str(8) # 计数0+未登记2 elif sum_num == 30: label = str(9) # 全未登记 else: label = str(10) # other return label def gouwu_func(x, y): ''' 组合-购物账户数据的处理 Args: x: string, 京东账号 y: string, 淘宝账号 Returns: string, 打标签-购物账号类别[0,1,2,3,4,5,6] ''' label_jd = one_feature(x) label_tb = one_feature(y) sum_num = label_jd + label_tb if (sum_num \u0026lt;= 2) and (sum_num \u0026gt;= 0): label = str(sum_num) # 正常计数 elif sum_num == 11: label = str(3) # 计数1+未登记1 elif sum_num == 10: label = str(4) # 计数0+未登记1 elif sum_num == 20: label = str(5) # 全未登记 else: label = str(6) # other return label def muti_family_func(marriage, is_child, child_sum, support_pre_count): ''' 组合 - 家庭结构处理 Args: marriage: number, 婚姻状态 is_child: number, 是否子女 child_sum: number, 孩子数量 support_pre_count: number, 供养人数 Returns: string, 打标签-家庭结构类别[0,1,2,3,4] ''' if ((support_pre_count \u0026lt;= 6) \u0026amp; (is_child \u0026lt; 2) \u0026amp; (marriage \u0026lt; 4) \u0026amp; (child_sum \u0026lt; 5)): if ((marriage == 0) | ((marriage == 2) \u0026amp; (is_child == 1)) | ((marriage == 3) \u0026amp; (is_child == 1))): label = str(0) # 单身家庭 elif (((marriage == 1) \u0026amp; (is_child == 1)) | ( (marriage == 1) \u0026amp; (is_child == 0) \u0026amp; ((support_pre_count == 0) | (support_pre_count \u0026lt;= child_sum)))): label = str(1) # 夫妻二人 elif ((marriage == 1) \u0026amp; (is_child == 0) \u0026amp; (support_pre_count \u0026gt;= child_sum)): label = str(2) # 核心家庭和主干家庭 elif ((marriage == 2) \u0026amp; (is_child == 0) \u0026amp; (child_sum \u0026gt;= 1) \u0026amp; (support_pre_count \u0026gt;= child_sum)): label = str(3) # 单亲家庭 else: label = str(4) # other else: label = str(5) # other,error_data return label def month_income(pd_series=dataset2['month_income'], divition=[float('-inf'), 3000, 30000, 80000, 150000, 300000, 1000000, 5000000, float('inf')], labels=['0', '1', '2', '3', '4', '5', '6', '7']): ''' 单列 - 月收入处理 Args: pd_series: pd.Series,里面的元素number, 欲进行分箱数据 divition: list,里面元素number, 用于分箱的临界值数据 labels: Returns:pd.Series, 分箱之后的标签结果.['0', '1', '2', '3', '4', '5', '6', '7']分别对应 贫困人口,穷人,低收入,初级小康,高级小康,中高收入,高收入,三者归一类（富翁,富豪,超级富豪） ''' bins_series = pd.cut(pd_series* 12, bins=divition, right=True, precision=1, retbins=False, labels=labels) return bins_series def income_cata(outcome, income): ''' 组合-辅助1，家庭支出和月收入 用于对家庭支出、月收入各单列中的数据进行处理，为计算家庭支出比重income_ratio函数做准备。 Args: outcome: number, 家庭支出 income: number,　家庭收入 Returns:string, 打标签-辅助1类别[0,1,2,3] ''' if (outcome \u0026lt;= 0.0) and (income \u0026lt;= 0): label = str(1) # 表示家庭支出、月收入，全部\u0026lt;=0的情况 elif (outcome \u0026lt;= 0.0) and (income \u0026gt; 0): label = str(2) # 表示家庭支出\u0026lt;=0的情况 elif (outcome \u0026gt; 0.0) and (income \u0026lt;= 0): label = str(3) # 表示月收入\u0026lt;=0的情况 else: label = str(0) # 表示家庭支出、月收入全部\u0026gt;0的情况 return label def income_ratio(outcome, income): ''' 组合-比值，家庭支出和月收入 Args: outcome: number, 家庭支出 income: number,　家庭收入 Returns:number, 家庭支出比重[0,inf],inf表示无穷 ''' label = income_cata(outcome, income) if label == '0': x = float(outcome) y = float(income) value = x / y elif label == '1': value = 1 elif label == '2': value = 0 else: value = 100 # 对应于income_cata返回结果3，即家庭收入\u0026lt;=0的情况，赋给指定值100。 return value def income_ratio_addcata(outcome, income): ''' 组合-比值衍生类别1，家庭支出和月收入 衍生的类别变量，和家庭支出、月收入组合特征共同使用 Args: outcome: number, 家庭支出 income: number,　家庭收入 Returns:string, 打标签-支出比重的类别[0,1,2,3,4,5] ''' value = income_ratio(outcome, income) if value == 0: label = str(0) # 表示家庭支出比重=0的情况 elif (value \u0026gt; 0) \u0026amp; (value \u0026lt;= .01): label = str(1) # 表示家庭支出比重(0,0.01]的情况,’(]‘表示左开右闭 elif (value \u0026gt; 0.01) \u0026amp; (value \u0026lt;= .1): label = str(2) # 表示家庭支出比重(0.01,0.1]的情况 elif (value \u0026gt; 0.1) \u0026amp; (value \u0026lt;= 1): label = str(3) # 表示家庭支出比重(0.1,1]的情况 elif (value \u0026gt; 1) \u0026amp; (value \u0026lt;= 10): label = str(4) # 表示家庭支出比重(1,10]的情况 else: label = str(5) # 表示家庭支出比重(10,inf]的情况,inf表示无穷 return label def cd_flag(x): ''' 目标变量的处理 逾期数＞0即认为逾期 Args: x: number, 目标变量数值 Returns:string, 打标签-目标变量类别[0,1] ''' if x \u0026gt; 0: label = str(1) # 1表示逾期 else: label = str(0) # 0表示正常 return label  目标变量处理（逾期数cd） dataset2['cd'] = dataset2['cd'].apply(cd_flag)  组合-微信、微博、qq账户数量：社交数量shejiao_num 对缺失的类别保留，并相应处理\ndataset2['shejiao_num'] = list(map(lambda x,y,z: shejiao_func(x,y,z), dataset2['weixin'],dataset2['weibo'],dataset2['qq']))  dataset2.groupby(['cd','shejiao_num']).agg({'intopieces_id':'count'}).unstack()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; }     intopieces_id   shejiao_num 0 1 2 3 4 5 6 7 8 9   cd               0 1126 691 538 1084 89 115332 29575 185 33 784481   1 734 268 128 201 37 62621 18125 131 11 359495     # fig1 = plt.figure() # fig1.add_subplot(111) dataset2.groupby(['cd','shejiao_num']).agg({'intopieces_id':'count'}).unstack().T.plot(kind='bar')  \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x37e4c1d0\u0026gt;  dataset2.groupby(['cd','shejiao_num']).agg({'intopieces_id':'count'}).unstack()/dataset2.groupby(['cd','shejiao_num']).agg({'intopieces_id':'count'}).unstack().sum()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; }     intopieces_id   shejiao_num 0 1 2 3 4 5 6 7 8 9   cd               0 0.605376 0.720542 0.807808 0.84358 0.706349 0.648104 0.620021 0.585443 0.75 0.68575   1 0.394624 0.279458 0.192192 0.15642 0.293651 0.351896 0.379979 0.414557 0.25 0.31425     shejiao_perc = dataset2.groupby(['cd','shejiao_num']).agg({'intopieces_id':'count'}).unstack()/dataset2.groupby(['cd','shejiao_num']).agg({'intopieces_id':'count'}).unstack().sum() shejiao_perc.T.plot(kind='bar',stacked=True)  \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x26f18dd8\u0026gt;  从上面观察考虑，认为同时拥有3个账号的客户，其所逾期的可能性会更低。同时怀疑，是否存在某一个账号是影响的关键？\n组合-京东账号、淘宝账号：购物账号数量gouwu_num dataset2['gouwu_num'] = list(map(lambda x,y: gouwu_func(x,y), dataset2['alipay_account'],dataset2['jd_account']))  dataset2.groupby(['cd','gouwu_num']).agg({'intopieces_id':'count'}).unstack().T.plot(kind='bar')  \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x3bd39c50\u0026gt;  dataset2.groupby(['cd','gouwu_num']).agg({'intopieces_id':'count'}).unstack()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; }     intopieces_id   gouwu_num 0 1 2 3 4 5   cd           0 1052 354 1331 5774 8 924615   1 308 60 181 2023 4 439175     dataset2.groupby(['cd','gouwu_num']).agg({'intopieces_id':'count'}).unstack()/dataset2.groupby(['cd','gouwu_num']).agg({'intopieces_id':'count'}).unstack().sum()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; }     intopieces_id   gouwu_num 0 1 2 3 4 5   cd           0 0.773529 0.855072 0.880291 0.740541 0.666667 0.677975   1 0.226471 0.144928 0.119709 0.259459 0.333333 0.322025     gouwu_perc = dataset2.groupby(['cd','gouwu_num']).agg({'intopieces_id':'count'}).unstack()/dataset2.groupby(['cd','gouwu_num']).agg({'intopieces_id':'count'}).unstack().sum() gouwu_perc.T.plot(kind='bar',stacked=True)  \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x2ea68320\u0026gt;  组合-marriage、is_child、child_sum、support_pre_count：家庭结构hunyin_cata dataset2['child_sum'].unique()  array([ 0., 2., 1., 3., 4., nan, 5., 8., 62., 6., 7., 11., 68., 100., 22., 15., 12., 25., 113., 20., 10., 41., 33., 14., 9., 21., 24., 23.])  dataset2['hunyin_cata'] = list(map(lambda x,y,z,w: muti_family_func(x,y,z,w), dataset2['marriage'],dataset2['is_child'],dataset2['child_sum'],dataset2['support_pre_count'])) # marriage, is_child, child_sum, support_pre_count  dataset2.groupby(['hunyin_cata']).count()['intopieces_id']  hunyin_cata 0 2452 1 209424 2 1909 3 451980 4 625078 5 84042 Name: intopieces_id, dtype: int64  dataset2.groupby(['cd','hunyin_cata']).agg({'intopieces_id':'count'}).unstack().T.plot(kind='bar')  \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x43d76b38\u0026gt;  dataset2.groupby(['cd','hunyin_cata']).agg({'intopieces_id':'count'}).unstack()/dataset2.groupby(['cd','hunyin_cata']).agg({'intopieces_id':'count'}).unstack().sum()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; }     intopieces_id   hunyin_cata 0 1 2 3 4 5   cd           0 0.790783 0.67412 0.641173 0.610206 0.744593 0.565955   1 0.209217 0.32588 0.358827 0.389794 0.255407 0.434045     hunyin_perc = dataset2.groupby(['cd','hunyin_cata']).agg({'intopieces_id':'count'}).unstack()/dataset2.groupby(['cd','hunyin_cata']).agg({'intopieces_id':'count'}).unstack().sum() hunyin_perc.T.plot(kind='bar',stacked=True)  \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x2ead0eb8\u0026gt;  单列-月收入数据：收入分级monthincome_bins dataset2['monthincome_bins'] = month_income(dataset2['month_income'])  dataset2.groupby(['monthincome_bins']).count()['intopieces_id']  monthincome_bins 0 93175 1 26904 2 475463 3 291892 4 191145 5 209742 6 74058 7 12506 Name: intopieces_id, dtype: int64  dataset2.groupby(['cd','monthincome_bins']).agg({'intopieces_id':'count'}).unstack()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; }     intopieces_id   monthincome_bins 0 1 2 3 4 5 6 7   cd             0 50840 18555 322996 204990 134515 147132 46699 7407   1 42335 8349 152467 86902 56630 62610 27359 5099     # fig7 = plt.figure() # fig7.add_subplot(111) dataset2.groupby(['cd','monthincome_bins']).agg({'intopieces_id':'count'}).unstack().T.plot(kind='bar')  \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x2eb27470\u0026gt;  dataset2.groupby(['cd','monthincome_bins']).agg({'intopieces_id':'count'}).unstack()/dataset2.groupby(['cd','monthincome_bins']).agg({'intopieces_id':'count'}).unstack().sum()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; }     intopieces_id   monthincome_bins 0 1 2 3 4 5 6 7   cd             0 0.54564 0.689674 0.679329 0.70228 0.703733 0.70149 0.630573 0.592276   1 0.45436 0.310326 0.320671 0.29772 0.296267 0.29851 0.369427 0.407724     monthincome_perc = dataset2.groupby(['cd','monthincome_bins']).agg({'intopieces_id':'count'}).unstack()/dataset2.groupby(['cd','monthincome_bins']).agg({'intopieces_id':'count'}).unstack().sum() monthincome_perc.T.plot(kind='bar',stacked=True)  \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x2ecdb748\u0026gt;  组合-月收入、家庭支出：家庭支出比重zhichu_ratio zhichu_ratio生成和分布情况观察 dataset2[dataset2['support_pre_pay']\u0026gt;dataset2['month_income']].head()  dataset2['zhichu_ratio'] = list(map(lambda x,y: income_ratio(x,y), dataset2['support_pre_pay'],dataset2['month_income']))  值分布图 fig81 = plt.figure(figsize=(16,10)) fig81.suptitle('家庭月支出比重（值分布）') plt.tight_layout(pad=2) #设置默认的间距 plt.subplots_adjust(wspace=0.5, hspace=0.5) fig81.add_subplot(241) plot1 = sns.boxplot(y='zhichu_ratio',x='cd',data=dataset2) plot1.set(ylim=(0,1)) plt.title('ALL') fig81.add_subplot(242) plot1 = sns.boxplot(y='zhichu_ratio',x='cd',data=dataset2[(dataset2['zhichu_ratio']== 0)]) plt.title('= 0') fig81.add_subplot(243) plot1 = sns.boxplot(y='zhichu_ratio',x='cd',data=dataset2[(dataset2['zhichu_ratio']\u0026gt; 0) \u0026amp; ((dataset2['zhichu_ratio']\u0026lt;=.01))]) plot1.set(ylim=(0,.01)) plt.title('0-0.01') fig81.add_subplot(244) plot1 = sns.boxplot(y='zhichu_ratio',x='cd',data=dataset2[(dataset2['zhichu_ratio']\u0026gt;.01) \u0026amp; (dataset2['zhichu_ratio']\u0026lt;=0.1)]) plot1.set(ylim=(0.01,0.1)) plt.title('0.01-0.1') fig81.add_subplot(245) plot1 = sns.boxplot(y='zhichu_ratio',x='cd',data=dataset2[(dataset2['zhichu_ratio']\u0026gt;0.1) \u0026amp; (dataset2['zhichu_ratio']\u0026lt;=1)]) plot1.set(ylim=(0,1)) plt.title('0.1-1') fig81.add_subplot(246) plot1 = sns.boxplot(y='zhichu_ratio',x='cd',data=dataset2[(dataset2['zhichu_ratio']\u0026gt;1) \u0026amp; (dataset2['zhichu_ratio']\u0026lt;=10)]) plot1.set(ylim=(0,10)) plt.title('1-10') fig81.add_subplot(247) plot1 = sns.boxplot(y='zhichu_ratio',x='cd',data=dataset2[(dataset2['zhichu_ratio']\u0026gt;10) \u0026amp; (dataset2['zhichu_ratio']\u0026lt;100)]) plot1.set(ylim=(10,100)) plt.title('10-100') fig81.add_subplot(248) plot1 = sns.boxplot(y='zhichu_ratio',x='cd',data=dataset2[(dataset2['zhichu_ratio']\u0026gt;100)]) plot1.set(ylim=(100,5000)) plt.title('\u0026gt;100')  Text(0.5,1,'\u0026gt;100')  频数统计 fig82 = plt.figure(figsize=(16,10)) fig82.suptitle('家庭月支出比重（频数）') plt.tight_layout(pad=2) #设置默认的间距 plt.subplots_adjust(wspace=0.5, hspace=0.5) fig82.add_subplot(241) plot1 = sns.countplot(x='cd',data=dataset2) plt.title('ALL') fig82.add_subplot(242) plot1 = sns.countplot(x='cd',data=dataset2[(dataset2['zhichu_ratio']== 0)]) plt.title('= 0') fig82.add_subplot(243) plot1 = sns.countplot(x='cd',data=dataset2[(dataset2['zhichu_ratio']\u0026gt; 0) \u0026amp; ((dataset2['zhichu_ratio']\u0026lt;=.01))]) plt.title('0-0.01') fig82.add_subplot(244) plot1 = sns.countplot(x='cd',data=dataset2[(dataset2['zhichu_ratio']\u0026gt;.01) \u0026amp; (dataset2['zhichu_ratio']\u0026lt;=0.1)]) # plot1.set(ylim=(0,1)) plt.title('0.01-0.1') fig82.add_subplot(245) plot1 = sns.countplot(x='cd',data=dataset2[(dataset2['zhichu_ratio']\u0026gt;0.1) \u0026amp; (dataset2['zhichu_ratio']\u0026lt;=1)]) # plot1.set(ylim=(0,1)) plt.title('0.1-1') fig82.add_subplot(246) plot1 = sns.countplot(x='cd',data=dataset2[(dataset2['zhichu_ratio']\u0026gt;1) \u0026amp; (dataset2['zhichu_ratio']\u0026lt;=10)]) # plot1.set(ylim=(0,1)) plt.title('1-10') fig82.add_subplot(247) plot1 = sns.countplot(x='cd',data=dataset2[(dataset2['zhichu_ratio']\u0026gt;10) \u0026amp; (dataset2['zhichu_ratio']\u0026lt;100)]) # plot1.set(ylim=(10,100)) plt.title('10-100') fig82.add_subplot(248) plot1 = sns.countplot(x='cd',data=dataset2[(dataset2['zhichu_ratio']\u0026gt;100)]) # plot1.set(ylim=(100,5000)) plt.title('\u0026gt;100')  Text(0.5,1,'\u0026gt;100')  结论与建议 结合原始数据（月收入month_income、家庭支出support_pre_pay），其数值存在超过10万以上的，一部分可能原因是线下录入有误，一部分原因可能是客户造假，其余部分可能是存在极个别客户填写数值是真实的。基于此考虑，在对两列处理之后，相应会存在比值大于100、以及=0情况，从箱线图结果观察到＞100的部分，正常与逾期的分布无明显区别。因此，针对收入比重大于100的部分，可以考虑去除。\n尝试（log变换） dataset2['temp_log'] = dataset2['zhichu_ratio'].apply('log')  C:\\Users\\M4500\\Anaconda3\\envs\\py35_xgboost\\lib\\site-packages\\pandas\\core\\base.py:307: RuntimeWarning: divide by zero encountered in log return f(self, *args, **kwargs)  值分布 fig84 = plt.figure(figsize=(16,8)) fig84.suptitle('家庭月支出log变换') plt.tight_layout(pad=2) #设置默认的间距 plt.subplots_adjust(wspace=0.5, hspace=0.5) fig84.add_subplot(241) plot1 = sns.boxplot(y='temp_log',x='cd',data=dataset2) # plot1.set(ylim=(0,1)) plt.title('ALL') fig84.add_subplot(242) plot1 = sns.boxplot(y='temp_log',x='cd',data=dataset2[(dataset2['zhichu_ratio']== 0)]) plt.title('= 0') fig84.add_subplot(243) plot1 = sns.boxplot(y='temp_log',x='cd',data=dataset2[(dataset2['zhichu_ratio']\u0026gt; 0) \u0026amp; ((dataset2['zhichu_ratio']\u0026lt;=.01))]) # plot1.set(ylim=(0,.01)) plt.title('0-0.01') fig84.add_subplot(244) plot1 = sns.boxplot(y='temp_log',x='cd',data=dataset2[(dataset2['zhichu_ratio']\u0026gt;.01) \u0026amp; (dataset2['zhichu_ratio']\u0026lt;=0.1)]) # plot1.set(ylim=(0.01,0.1)) plt.title('0.01-0.1') fig84.add_subplot(245) plot1 = sns.boxplot(y='temp_log',x='cd',data=dataset2[(dataset2['zhichu_ratio']\u0026gt;0.1) \u0026amp; (dataset2['zhichu_ratio']\u0026lt;=1)]) # plot1.set(ylim=(0,1)) plt.title('0.1-1') fig84.add_subplot(246) plot1 = sns.boxplot(y='temp_log',x='cd',data=dataset2[(dataset2['zhichu_ratio']\u0026gt;1) \u0026amp; (dataset2['zhichu_ratio']\u0026lt;=10)]) # plot1.set(ylim=(0,10)) plt.title('1-10') fig84.add_subplot(247) plot1 = sns.boxplot(y='temp_log',x='cd',data=dataset2[(dataset2['zhichu_ratio']\u0026gt;10) \u0026amp; (dataset2['zhichu_ratio']\u0026lt;100)]) # plot1.set(ylim=(10,100)) plt.title('10-100') fig84.add_subplot(248) plot1 = sns.boxplot(y='temp_log',x='cd',data=dataset2[(dataset2['zhichu_ratio']\u0026gt;100)]) # plot1.set(ylim=(100,5000)) plt.title('\u0026gt;100')  C:\\Users\\M4500\\Anaconda3\\envs\\py35_xgboost\\lib\\site-packages\\matplotlib\\cbook\\__init__.py:1847: RuntimeWarning: invalid value encountered in double_scalars stats['iqr'] = q3 - q1 C:\\Users\\M4500\\Anaconda3\\envs\\py35_xgboost\\lib\\site-packages\\matplotlib\\cbook\\__init__.py:1872: RuntimeWarning: invalid value encountered in less_equal wiskhi = np.compress(x \u0026lt;= hival, x) C:\\Users\\M4500\\Anaconda3\\envs\\py35_xgboost\\lib\\site-packages\\matplotlib\\cbook\\__init__.py:1879: RuntimeWarning: invalid value encountered in greater_equal wisklo = np.compress(x \u0026gt;= loval, x) C:\\Users\\M4500\\Anaconda3\\envs\\py35_xgboost\\lib\\site-packages\\numpy\\lib\\function_base.py:4406: RuntimeWarning: invalid value encountered in multiply x2 = take(ap, indices_above, axis=axis) * weights_above Text(0.5,1,'\u0026gt;100')  结论与建议——（不推荐） 由上面的图表观察，log变换带来数据的区分度并不理想。因此，暂且不考虑log变换。\nzhichu_ratio的衍生特征——zhichu_ratio_addcata dataset2['zhichu_ratio_addcata'] = list(map(lambda x,y: income_ratio_addcata(x, y), dataset2['support_pre_pay'],dataset2['month_income']))  fig85 = plt.figure(figsize=(8,8)) plt.tight_layout(pad=2) #设置默认的间距 plt.subplots_adjust(wspace=0.5, hspace=0.5) fig85.add_subplot(121) plot1 = sns.countplot(x='zhichu_ratio_addcata',hue='cd',data=dataset2, dodge=False) # plot1.set(ylim=(0,1)) plt.title('ALL')  Text(0.5,1,'ALL')  变量重要性判别 从统计相关的角度分析单变量的影响 数据集中的社交数据和购物数据可认为是数值型，即连续-离散的变量分析\n社交和购物数据视为连续—离散 # dataset2.info()  # from sklearn.feature_selection import SelectKBest # from sklearn.feature_selection import f_classif # 即假设检验-anova方法 # k_best = SelectKBest(f_classif, k='all') # Kbest = k_best.fit(dataset2[['gouwu_num','shejiao_num']], dataset2[['cd']]) # Kbest.pvalues_  从上面的结果，p\u0026lt;0.05，故认为两个变量对结果均有影响\n社交和购物数据视为离散—离散 from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import chi2 dataset2_sub = dataset2[['gouwu_num','shejiao_num','cd']] dataset2_sub['gouwu_num'] = dataset2_sub['gouwu_num'].astype(str) dataset2_sub['shejiao_num'] = dataset2_sub['shejiao_num'].astype(str) #选择K个最好的特征，返回选择特征后的数据 model1 = SelectKBest(chi2, k='all') model1.fit_transform(dataset2_sub[['gouwu_num','shejiao_num']], dataset2_sub['cd'])[1] # 各特征的得分 model1.scores_, dataset2_sub[['gouwu_num','shejiao_num']].columns  C:\\Users\\M4500\\Anaconda3\\envs\\py35_xgboost\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy C:\\Users\\M4500\\Anaconda3\\envs\\py35_xgboost\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy import sys (array([ 5.31019853, 341.66495808]), Index(['gouwu_num', 'shejiao_num'], dtype='object'))  # 各特征的p值 model1.pvalues_ , dataset2_sub[['gouwu_num','shejiao_num']].columns  (array([2.12009402e-02, 2.76869376e-76]), Index(['gouwu_num', 'shejiao_num'], dtype='object'))  从上面的结果，p\u0026lt;0.05，故认为两个变量对目标变量均有影响\n家庭支出比重（zhichu_ratio）的连续—离散 家庭支出比重（ALL） from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import f_classif # 即假设检验-anova方法 k_best = SelectKBest(f_classif, k='all') Kbest = k_best.fit(dataset2[['month_income','support_pre_pay', 'zhichu_ratio']], dataset2[['cd']]) Kbest.pvalues_  C:\\Users\\M4500\\Anaconda3\\envs\\py35_xgboost\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). y = column_or_1d(y, warn=True) array([4.99626685e-06, 9.29123318e-01, 2.43230145e-01])  从假设检验的分析结果中观察到，对于原始单列数据month_income，它对目标变量有一定影响；原始单列数据support_pre_pay、组合数据对目标变量无显著影响\n家庭支出比重（(0\u0026lt;ratio≤10） from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import f_classif # 即假设检验-anova方法 k_best = SelectKBest(f_classif, k='all') dataset2_sub3 = dataset2[(dataset2['zhichu_ratio']\u0026gt;0)\u0026amp;(dataset2['zhichu_ratio']\u0026lt;10)] Kbest = k_best.fit(dataset2_sub3[['month_income','support_pre_pay', 'zhichu_ratio']], dataset2_sub3[['cd']]) Kbest.pvalues_  C:\\Users\\M4500\\Anaconda3\\envs\\py35_xgboost\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). y = column_or_1d(y, warn=True) array([4.55373808e-06, 3.58681469e-01, 6.01862819e-03])  家庭支出比重（(0≤ratio≤10） from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import f_classif # 即假设检验-anova方法 k_best = SelectKBest(f_classif, k='all') dataset2_sub3 = dataset2[(dataset2['zhichu_ratio']\u0026gt;=0)\u0026amp;(dataset2['zhichu_ratio']\u0026lt;10)] Kbest = k_best.fit(dataset2_sub3[['month_income','support_pre_pay', 'zhichu_ratio']], dataset2_sub3[['cd']]) Kbest.pvalues_  C:\\Users\\M4500\\Anaconda3\\envs\\py35_xgboost\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). y = column_or_1d(y, warn=True) array([2.17489841e-06, 0.00000000e+00, 0.00000000e+00])  结论 分三种情况区别看待家庭支出比重，0，（0,10]，（10，inf）。在对整体范围数据进行检验时，zhichu_ratio不显著，即难以判别对目标变量的影响；当在＜10的情况下，zhichu_ratio显著，明显对目标变量有影响，尤其在=0的情况当中。 故建议添加一列特征作为辅助区分三种情况。\n从多变量结合角度 离散—离散 [\u0026lsquo;shejiao_num\u0026rsquo;,\u0026lsquo;gouwu_num\u0026rsquo;,\u0026lsquo;hunyin_cata\u0026rsquo;,\u0026lsquo;monthincome_bins\u0026rsquo;,\u0026lsquo;zhichu_ratio_addcata\u0026rsquo;] 社交、购物、家庭结构、收入分级、家庭比重（衍生类别）等离散变量判断。\nfrom sklearn.feature_selection import SelectKBest from sklearn.feature_selection import chi2 dataset2_sub = dataset2[['gouwu_num','shejiao_num','hunyin_cata','monthincome_bins','zhichu_ratio_addcata', 'cd']] dataset2_sub['gouwu_num'] = dataset2_sub['gouwu_num'].astype(str) dataset2_sub['shejiao_num'] = dataset2_sub['shejiao_num'].astype(str) #选择K个最好的特征，返回选择特征后的数据 model1 = SelectKBest(chi2, k='all') model1.fit_transform(dataset2_sub[['gouwu_num','shejiao_num','hunyin_cata','monthincome_bins','zhichu_ratio_addcata']], dataset2_sub['cd'])[1] # 各特征的得分 model1.scores_, dataset2_sub[['gouwu_num','shejiao_num','hunyin_cata','monthincome_bins','zhichu_ratio_addcata']].columns  C:\\Users\\M4500\\Anaconda3\\envs\\py35_xgboost\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy C:\\Users\\M4500\\Anaconda3\\envs\\py35_xgboost\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy import sys (array([5.31019853e+00, 3.41664958e+02, 5.59435501e+02, 1.38677840e+03, 6.14311476e+04]), Index(['gouwu_num', 'shejiao_num', 'hunyin_cata', 'monthincome_bins', 'zhichu_ratio_addcata'], dtype='object'))  # 各特征的p值 model1.pvalues_ , dataset2_sub[['gouwu_num','shejiao_num','hunyin_cata','monthincome_bins','zhichu_ratio_addcata']].columns  (array([2.12009402e-002, 2.76869376e-076, 1.11536417e-123, 1.56862997e-303, 0.00000000e+000]), Index(['gouwu_num', 'shejiao_num', 'hunyin_cata', 'monthincome_bins', 'zhichu_ratio_addcata'], dtype='object'))  采用高级统计方法识别特征效用 from sklearn.linear_model import RandomizedLogisticRegression r_logistic = RandomizedLogisticRegression(C=1, scaling=0.5, sample_fraction=0.75, n_resampling=100, # selection_thershold=0.1, fit_intercept=True, verbose=True, # normolize=True, random_state=1234, n_jobs=1)  C:\\Users\\M4500\\Anaconda3\\envs\\py35_xgboost\\lib\\site-packages\\sklearn\\utils\\deprecation.py:58: DeprecationWarning: Class RandomizedLogisticRegression is deprecated; The class RandomizedLogisticRegression is deprecated in 0.19 and will be removed in 0.21. warnings.warn(msg, category=DeprecationWarning)  r_logistic.fit(dataset2_sub[['gouwu_num','shejiao_num','hunyin_cata','monthincome_bins','zhichu_ratio_addcata']], dataset2_sub['cd'])  [Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed: 5.0min finished RandomizedLogisticRegression(C=1, fit_intercept=True, memory=None, n_jobs=1, n_resampling=100, normalize=True, pre_dispatch='3*n_jobs', random_state=1234, sample_fraction=0.75, scaling=0.5, selection_threshold=0.25, tol=0.001, verbose=True)  r_logistic.get_support()  array([ True, True, True, True, True])  r_logistic.all_scores_  array([[1. ], [0.9 ], [0.67], [1. ], [1. ]])  从结果观察可知，对于5个离散特征：[\u0026lsquo;gouwu_num\u0026rsquo;,\u0026lsquo;shejiao_num\u0026rsquo;,\u0026lsquo;hunyin_cata\u0026rsquo;,\u0026lsquo;monthincome_bins\u0026rsquo;,\u0026lsquo;zhichu_ratio_addcata\u0026rsquo;]除了\u0026rsquo;hunyin_cata\u0026rsquo;重要性较低之外，其他变量都重要性得分很高。对于连续特征\u0026rsquo;zhichu_ratio\u0026rsquo;,在单因素分析评估中影响显著。\n故可考虑将6个衍生特征视为重要特征使用。\n结论 综合以上的判别和评估，认为社交数量、购物账号数量、收入分级、家庭支出比重及其类型的重要性最为主要，其次是家庭结构。因此，认为所有的衍生特征均可考虑添加到模型当中进一步测试和评估它们的贡献程度。\n","date":1538659564,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538659564,"objectID":"82380495065a184229e18d26846b68f5","permalink":"https://growing-bison.github.io/post/%E5%8F%8D%E6%AC%BA%E8%AF%88%E5%AD%90%E7%A0%94%E7%A9%B6-%E7%89%B9%E5%BE%81%E8%A1%8D%E7%94%9F%E8%BF%87%E7%A8%8B/","publishdate":"2018-10-04T21:26:04+08:00","relpermalink":"/post/%E5%8F%8D%E6%AC%BA%E8%AF%88%E5%AD%90%E7%A0%94%E7%A9%B6-%E7%89%B9%E5%BE%81%E8%A1%8D%E7%94%9F%E8%BF%87%E7%A8%8B/","section":"post","summary":"Table of Contents 1\u0026nbsp;\u0026nbsp;衍生特征分析过程1.1\u0026nbsp;\u0026nbsp;数据读取1.2\u0026nbsp;\u0026nbsp;定义处理单列的数据1.2","tags":["python","特征工程"],"title":"反欺诈子研究-特征衍生过程","type":"post"},{"authors":[],"categories":["数据科学项目","数据挖掘","数据分析"],"content":" 数据说明\n1. 数据信息：\n- 数据量：40多万条观测，20多个列变量\n- 时间：2018年5月前\n2. 数据来源\n- 作者：田昕峣\n- 获取方式：https://github.com/XinyaoTian/lianjia_Spider\n项目目标\n- 建立单位面积房价的预测模型\n内容目录 1\u0026nbsp;\u0026nbsp;数据导入2\u0026nbsp;\u0026nbsp;数据探索：3\u0026nbsp;\u0026nbsp;数据处理：4\u0026nbsp;\u0026nbsp;绘图分析4.1\u0026nbsp;\u0026nbsp;单变量观察4.1.1\u0026nbsp;\u0026nbsp;面积、查看次数、收藏次数、发布时间4.1.2\u0026nbsp;\u0026nbsp;2年产权、5年产权、房屋户型4.1.3\u0026nbsp;\u0026nbsp;朝向、装修程度、电梯配备、楼层位置、楼型、建成时间4.2\u0026nbsp;\u0026nbsp;多维度分析4.2.1\u0026nbsp;\u0026nbsp;产权和查看次数、收藏次数4.2.2\u0026nbsp;\u0026nbsp;户型+产权和查看次数、收藏次数¶4.3\u0026nbsp;\u0026nbsp;关联分析4.3.1\u0026nbsp;\u0026nbsp;2年产权、5年产权vs装修程度、户型4.3.2\u0026nbsp;\u0026nbsp;电梯、楼层、楼型、建成时间单变量统计4.3.3\u0026nbsp;\u0026nbsp;连续变量的相关性4.3.4\u0026nbsp;\u0026nbsp;异常值检查（size_house_edit1与smeter_price_edit1关系为例）4.3.4.1\u0026nbsp;\u0026nbsp;检查4.3.4.2\u0026nbsp;\u0026nbsp;剔除异常点4.4\u0026nbsp;\u0026nbsp;目标变量处理——满足整体分布4.4.1\u0026nbsp;\u0026nbsp;绘制正态分布图4.4.2\u0026nbsp;\u0026nbsp;绘制QQ图4.4.3\u0026nbsp;\u0026nbsp;变换处理与查看4.5\u0026nbsp;\u0026nbsp;缺失值处理4.6\u0026nbsp;\u0026nbsp;其它特征工程4.6.1\u0026nbsp;\u0026nbsp;1、有许多特征实际上是类别型的特征，但给出来的是数字，所以需要将其转换成类别型。4.6.2\u0026nbsp;\u0026nbsp;2、接下来 LabelEncoder，对部分类别的特征进行编号。4.6.3\u0026nbsp;\u0026nbsp;3、检查变量的正态分布情况4.6.3.1\u0026nbsp;\u0026nbsp;检查4.6.3.2\u0026nbsp;\u0026nbsp;变换处理4.6.4\u0026nbsp;\u0026nbsp;哑变量处理5\u0026nbsp;\u0026nbsp;建立模型5.1\u0026nbsp;\u0026nbsp;数据准备5.2\u0026nbsp;\u0026nbsp;模型函数5.2.1\u0026nbsp;\u0026nbsp;模型函数设定5.2.1.1\u0026nbsp;\u0026nbsp;lasso模型5.2.1.2\u0026nbsp;\u0026nbsp;ENet模型5.2.1.3\u0026nbsp;\u0026nbsp;KRR模型5.2.1.4\u0026nbsp;\u0026nbsp;GBoost模型5.2.1.5\u0026nbsp;\u0026nbsp;xgboost模型5.2.1.6\u0026nbsp;\u0026nbsp;LightGBM模型5.2.2\u0026nbsp;\u0026nbsp;模型得分5.3\u0026nbsp;\u0026nbsp;模型融合5.3.1\u0026nbsp;\u0026nbsp;基模型融合5.3.2\u0026nbsp;\u0026nbsp;构建stacking averagd models的类5.3.3\u0026nbsp;\u0026nbsp;测试模型融合5.3.3.1\u0026nbsp;\u0026nbsp;stacking5.3.3.2\u0026nbsp;\u0026nbsp;xgboost5.3.3.3\u0026nbsp;\u0026nbsp;lightgbm5.3.4\u0026nbsp;\u0026nbsp;结果\n数据导入 import numpy as np import pandas as pd import matplotlib.pyplot as plt plt.rcParams['font.sans-serif'] = ['SimHei'] plt.rcParams['font.family']='sans-serif' # 解决负号是方块 %matplotlib notebook import seaborn as sns color = sns.color_palette() sns.set_style('darkgrid') import warnings def ignore_warn(*args, **kwargs): pass warnings.warn = ignore_warn import re from scipy import stats from scipy.stats import norm, skew pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output pd.set_option('display.max_columns',40) # 显示隐藏  dataset = pd.read_csv('./houseInfo.csv')  a = dataset.ix[1, 'info_cluster']  dataset.head(5)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    introduction_house community_house href_house unit_house size_house direction_house decoration_house elevator_house type_house years_house area_house interests_house watch_times submit_period years_period tax_free total_price smeter_price region info_cluster info_flood info_follow     0 电梯花园洋房，开发商精装修带家具家电，小区人车分流 麓山国际帕萨迪纳3组 https://cd.lianjia.com/ershoufang/106101085290... NaN NaN NaN NaN NaN NaN NaN 麓山 NaN NaN NaN NaN NaN 250.000 单价25492元/平米 cd | 2室2厅 | 98.07平米 | 南 | 其他 | 有电梯 高楼层(共9层)2008年建板塔结合 - 3人关注 / 共0次带看 / 2个月以前发布   1 天府新区麓山国际跃层洋房纯清水出售 麓山国际塞尔维蒙 https://cd.lianjia.com/ershoufang/106101067528... NaN NaN NaN NaN NaN NaN NaN 麓山 NaN NaN NaN NaN NaN 420.000 单价20389元/平米 cd | 叠拼别墅 | 5室1厅 | 206平米 | 南 | 其他 | 无电梯 上叠(共4层)2008年建暂无数据 - 36人关注 / 共2次带看 / 2个月以前发布   2 麓山国际半月湾跃层，户型通透采光良好楼距开阔视野好 麓山国际半月湾 https://cd.lianjia.com/ershoufang/106101136261... NaN NaN NaN NaN NaN NaN NaN 麓山 NaN NaN NaN NaN NaN 275.000 单价24512元/平米 cd | 2室2厅 | 112.19平米 | 东南 | 其他 高楼层(共16层)2013年建板楼 - 43人关注 / 共1次带看 / 1个月以前发布   3 中丝园 装修 套三单卫 带车位 ! 心怡中丝园 https://cd.lianjia.com/ershoufang/106101229408... NaN NaN NaN NaN NaN NaN NaN 麓山 NaN NaN NaN NaN NaN 193.000 单价22043元/平米 cd | 3室2厅 | 87.56平米 | 南 | 其他 | 有电梯 高楼层(共33层)2015年建板塔结合 - 1人关注 / 共0次带看 / 12天以前发布   4 麓山国际因特拉肯A区+套三双卫+对中庭+看湖带装修 麓山国际茵特拉肯A https://cd.lianjia.com/ershoufang/106101233740... NaN NaN NaN NaN NaN NaN NaN 麓山 NaN NaN NaN NaN NaN 300.000 单价23303元/平米 cd | 3室2厅 | 128.74平米 | 西南 | 其他 中楼层(共11层)2016年建板楼 - 0人关注 / 共0次带看 / 10天以前发布     数据探索：  查看数据集中的变量情况  dataset.info()  \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 474301 entries, 0 to 474300 Data columns (total 22 columns): introduction_house 474301 non-null object community_house 474301 non-null object href_house 474301 non-null object unit_house 38137 non-null object size_house 38137 non-null object direction_house 38137 non-null object decoration_house 38109 non-null object elevator_house 37093 non-null object type_house 38137 non-null object years_house 38100 non-null object area_house 474301 non-null object interests_house 38137 non-null object watch_times 38137 non-null object submit_period 38137 non-null object years_period 30543 non-null object tax_free 35260 non-null object total_price 474301 non-null float64 smeter_price 474301 non-null object region 474301 non-null object info_cluster 436164 non-null object info_flood 436164 non-null object info_follow 436164 non-null object dtypes: float64(1), object(21) memory usage: 79.6+ MB  dataset.describe()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    total_price     count 474301.000   mean 329.913   std 371.062   min 4.000   25% 143.000   50% 235.000   75% 390.000   max 60000.000     # 检查数据维度 print(\u0026quot;训练集特征前的size：\u0026quot;,dataset.shape)  训练集特征前的size： (474301, 22)  数据处理： def size_help_func(x): # pattern = re.compile(r'\\d+') # match = pattern.search(x) x = str(x) x = x.replace('平方米','') x = x.replace('平米','') x = x.replace('米','') if (('室' in x) | ('厅' in x)|(x=='nan')|('车位' in x)|('房' in x)|('墅' in x)): num = 0 else: num = float(x) # if ('米' in (x))==True: # x = x.replace('平米','') # num = float(x.strip()) # else: # num = 0 return num def info_func(x): if '平米' in str(x): a = x.split('平米')[0].split('|')[-1].strip() if len(a)\u0026gt;1 : num = a else: num = 0.0 else: num = 0.0 return num def size_func(x,y): a = size_help_func(x) b = info_func(y) if a == 0.0: if ('车位' not in str(b)): num = float(b) else: num = a else: num =a return num def size_addcata_func(a): # a = watch_time_func(x) if a \u0026lt;= 10: label = str(1) else: label = str(0) return label def watch_time_func(x): if str(x) == 'nan': num = -1 else: a = x.split('次')[0].strip() num = int(a) return num def watch_time_addcata_func(x): # a = watch_time_func(x) if x == -1: label = str(1) else: label = str(0) return label def interests_house_func(x): if str(x) == 'nan': num = -1 else: a = x.split('人')[0].strip() num = int(a) return num def interests_house_addcata_func(x): # a = interests_house_func(x) if x == -1: label = str(1) else: label = str(0) return label def submit_period_func(x): if str(x) == 'nan': num = -1 elif '刚刚' in str(x): num = 0 elif '年' in str(x): a = x.split('年')[0].strip() if a == '一': num = 365 elif a == '二': num = 730 else: num = 1000 elif '个月' in x: a = x.split('个月')[0].strip() num = int(a)* 30 elif '天' in x: a = x.split('天')[0].strip() num = int(a) else: num = -2 return num def submit_period_addcata_func(x): a = submit_period_func(x) if a == -2: label = 3 elif a == -1: label = 2 elif a == 1000: label = 1 else: label = 0 return str(label) def years_period_func(x): if str(x) == 'nan': label = str(0) else: label = str(1) return label # def tax_free_func(x): # if str(x) == 'nan': # label = str(0) # else: # label = str(1) # return label def smeter_price_func(x): a = x.split('元')[0].replace('单价','') if len(a) \u0026lt;= 3: num = -1 else: num = int(a) return num def direction_func(x,y,z): x = str(x) y = str(y) z = str(z) dir_list = ['东','西','南','北'] if ((dir_list[0] in x)|(dir_list[1] in x)|(dir_list[2] in x)|(dir_list[3] in x)): label = x elif (dir_list[0] in y)|(dir_list[1] in y)|(dir_list[2] in y)|(dir_list[3] in y): label = y elif (dir_list[0] in z)|(dir_list[1] in z)|(dir_list[2] in z)|(dir_list[3] in z): a = z.split('|') for value in a: if (dir_list[0] in value)|(dir_list[1] in value)|(dir_list[2] in value)|(dir_list[3] in value): label = value else: label = 'nodata' else: label = 'nodata' return label def decoration_func(x,y,z): x = str(x) y = str(y) z = str(z) dir_list = ['精装', '其他', '毛坯', '简装'] if ((dir_list[0] in x)|(dir_list[1] in x)|(dir_list[2] in x)|(dir_list[3] in x)): label = x.strip() elif (dir_list[0] in y)|(dir_list[1] in y)|(dir_list[2] in y)|(dir_list[3] in y): label = y.strip() elif (dir_list[0] in z)|(dir_list[1] in z)|(dir_list[2] in z)|(dir_list[3] in z): a = z.split('|') for value in a: if (dir_list[0] in value)|(dir_list[1] in value)|(dir_list[2] in value)|(dir_list[3] in value): label = value.strip() else: label = 'nodata' else: label = 'nodata' return label def elevator_func(x,y,z): ''' x-decoration_house y-elevator_house z-info_cluster ''' x = str(x) y = str(y) z = str(z) dir_list = ['有电梯', '无电梯'] if (dir_list[0] in x)|(dir_list[1] in x): label = x.strip() elif (dir_list[0] in y)|(dir_list[1] in y): label = y.strip() elif (dir_list[0] in z)|(dir_list[1] in z): a = z.split('|') for value in a: if (dir_list[0] in value)|(dir_list[1] in value): label = value.strip() else: label = 'nodata' else: label = 'nodata' return label def floor_type_func(x): x = str(x) if '共' in x: a = x.split('(')[0] label = a elif '层' in x: a = x.split('层')[0] a = int(a) if a \u0026lt;=1: label = '底层' elif (a \u0026gt;1)|(a\u0026lt;6): label = '低楼层' elif (a \u0026gt;=6)|(a\u0026lt;15): label = '中楼层' else: label = '高楼层' elif '平房' in x: label = '底层' elif x == 'nan': label = 'nodata' else: label = 'nodata' return label def years_house_type_func(x,y): x = str(x) y = str(y) type_list = ['板塔','板','塔','平房','叠'] if (type_list[0] in x)|(type_list[0] in y): label = '板塔' elif (type_list[1] in x)|(type_list[1] in y): label = '板' elif (type_list[2] in x)|(type_list[2] in y): label = '塔' elif (type_list[3] in x)|(type_list[3] in y): label = '平房' elif (type_list[4] in x)|(type_list[4] in y): label = '别墅' else: label = 'nodata' return label def years_house_year_func(x,y): x = str(x) y = str(y) if ('年' in x): a = x.split('年')[0].replace('\\'','').strip() num = int(a) elif ('年' in y): a = y.split('年')[0].replace('\\'','').strip() num = int(a) else: num = None return num  dataset.columns  Index(['introduction_house', 'community_house', 'href_house', 'unit_house', 'size_house', 'direction_house', 'decoration_house', 'elevator_house', 'type_house', 'years_house', 'area_house', 'interests_house', 'watch_times', 'submit_period', 'years_period', 'tax_free', 'total_price', 'smeter_price', 'region', 'info_cluster', 'info_flood', 'info_follow'], dtype='object')  dataset['submit_period'].apply(submit_period_func)  0 -1 1 -1 2 -1 3 -1 4 -1 5 -1 6 -1 7 -1 8 -1 9 -1 10 -1 11 -1 12 -1 13 -1 14 -1 15 -1 16 -1 17 -1 18 -1 19 -1 20 -1 21 -1 22 -1 23 -1 24 -1 25 -1 26 -1 27 -1 28 -1 29 -1 .. 474271 -1 474272 -1 474273 -1 474274 -1 474275 -1 474276 -1 474277 -1 474278 -1 474279 -1 474280 -1 474281 -1 474282 -1 474283 -1 474284 -1 474285 -1 474286 -1 474287 -1 474288 -1 474289 -1 474290 -1 474291 -1 474292 -1 474293 -1 474294 -1 474295 -1 474296 -1 474297 -1 474298 -1 474299 -1 474300 -1 Name: submit_period, Length: 474301, dtype: int64  dataset['years_house_year_edit1'] = list(map(lambda x, y: years_house_year_func(x,y), dataset['type_house'],dataset['years_house']))  dataset['size_house_edit1'] = list(map(lambda x, y: size_func(x,y), dataset['unit_house'],dataset['info_cluster'])) dataset['size_house_edit1_addcata'] = dataset['size_house_edit1'].apply(size_addcata_func) dataset['watch_time_edit1'] = dataset['watch_times'].apply(watch_time_func) dataset['watch_time_edit1_addcata'] = dataset['watch_time_edit1'].apply(watch_time_addcata_func) dataset['interests_house_edit1'] = dataset['interests_house'].apply(interests_house_func) dataset['interests_house_edit1_addcata'] = dataset['interests_house_edit1'].apply(interests_house_addcata_func) dataset['submit_period_edit1'] = dataset['submit_period'].apply(submit_period_func) dataset['submit_period_edit1_addcata'] = dataset['submit_period'].apply(submit_period_addcata_func) dataset['years_period_edit1'] = dataset['years_period'].apply(years_period_func) dataset['tax_free_edit1'] = dataset['tax_free'].apply(years_period_func) dataset['smeter_price_edit1'] = dataset['smeter_price'].apply(smeter_price_func) dataset['direction_edit1'] = list(map(lambda x, y, z: direction_func(x,y,z), dataset['direction_house'],dataset['decoration_house'],dataset['info_cluster'])) dataset['decoration_edit1'] = list(map(lambda x, y, z: decoration_func(x,y,z), dataset['direction_house'],dataset['decoration_house'],dataset['info_cluster'])) dataset['elevator_edit1'] = list(map(lambda x, y, z: elevator_func(x,y,z), dataset['decoration_house'],dataset['elevator_house'],dataset['info_cluster'])) dataset['type_house_edit1'] = dataset['type_house'].apply(floor_type_func) dataset['years_house_type_edit1'] = list(map(lambda x, y: years_house_type_func(x,y), dataset['type_house'],dataset['years_house'])) dataset['years_house_year_edit1'] = list(map(lambda x, y: years_house_year_func(x,y), dataset['type_house'],dataset['years_house']))  print('房屋小区类型：', len(dataset['community_house'].unique())); print('房屋户型：', len(dataset['unit_house'].unique())) print('房屋面积：', 'max:',max(dataset['size_house_edit1'].unique()), 'min:',min(dataset['size_house_edit1'].unique()), '空值：',len(dataset[dataset['size_house_edit1']==0.])) print('房屋朝向：', len(dataset['direction_house'].unique())) print('看房次数：', 'max:',max(dataset['watch_time_edit1'].unique()), 'min:',min(dataset['watch_time_edit1'].unique()), '空值：',len(dataset[dataset['watch_time_edit1']==-1])) print('收藏次数：', 'max:',max(dataset['interests_house_edit1'].unique()), 'min:',min(dataset['interests_house_edit1'].unique()), '空值：',len(dataset[dataset['interests_house_edit1']==-1])) print('多久前发布：', 'max:',max(dataset['submit_period_edit1'].unique()), 'min:',min(dataset['submit_period_edit1'].unique()), '空值：',len(dataset[dataset['submit_period_edit1']==-1])) print('多久前发布的类型：', len(dataset['submit_period_edit1_addcata'].unique())) print('2年产权类型：', len(dataset['years_period_edit1'].unique())) print('5年产权类型：', len(dataset['tax_free_edit1'].unique())) print('总价：', 'max:',max(dataset['total_price'].unique()), 'min:',min(dataset['total_price'].unique()), '空值：',len(dataset[dataset['total_price']==-1])) print('单位价钱：', 'max:',max(dataset['smeter_price_edit1'].unique()), 'min:',min(dataset['smeter_price_edit1'].unique()), '空值：',len(dataset[dataset['smeter_price_edit1']==-1]))  房屋小区类型： 55148 房屋户型： 77 房屋面积： max: 12017.0 min: 0.0 空值： 67263 房屋朝向： 203 看房次数： max: 851 min: -1 空值： 436164 收藏次数： max: 2701 min: -1 空值： 436164 多久前发布： max: 365 min: -1 空值： 436164 多久前发布的类型： 2 2年产权类型： 2 5年产权类型： 2 总价： max: 60000.0 min: 4.0 空值： 0 总价： max: 199984 min: -1 空值： 2  dataset[dataset['direction_house']=='东 南 西 北'][['info_cluster','direction_house','decoration_house','info_flood']]   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    info_cluster direction_house decoration_house info_flood     337737 NaN 东 南 西 北 精装 NaN   342204 NaN 东 南 西 北 其他 NaN   342205 NaN 东 南 西 北 毛坯 NaN   342234 NaN 东 南 西 北 精装 NaN   353429 NaN 东 南 西 北 有电梯 NaN   353431 NaN 东 南 西 北 有电梯 NaN   353447 NaN 东 南 西 北 无电梯 NaN   354963 NaN 东 南 西 北 精装 NaN   354965 NaN 东 南 西 北 精装 NaN   359978 NaN 东 南 西 北 精装 NaN   360325 NaN 东 南 西 北 精装 NaN   360482 NaN 东 南 西 北 精装 NaN   365587 NaN 东 南 西 北 其他 NaN   365752 NaN 东 南 西 北 简装 NaN   366365 NaN 东 南 西 北 简装 NaN   367228 NaN 东 南 西 北 其他 NaN   371544 NaN 东 南 西 北 精装 NaN   371572 NaN 东 南 西 北 毛坯 NaN   371614 NaN 东 南 西 北 精装 NaN   371670 NaN 东 南 西 北 精装 NaN   371926 NaN 东 南 西 北 简装 NaN   371928 NaN 东 南 西 北 其他 NaN   371951 NaN 东 南 西 北 精装 NaN   371971 NaN 东 南 西 北 精装 NaN   371972 NaN 东 南 西 北 精装 NaN   372001 NaN 东 南 西 北 精装 NaN   372042 NaN 东 南 西 北 毛坯 NaN   372058 NaN 东 南 西 北 精装 NaN   372074 NaN 东 南 西 北 毛坯 NaN   372087 NaN 东 南 西 北 精装 NaN   ... ... ... ... ...   457320 NaN 东 南 西 北 精装 NaN   457803 NaN 东 南 西 北 简装 NaN   458709 NaN 东 南 西 北 简装 NaN   458729 NaN 东 南 西 北 简装 NaN   458986 NaN 东 南 西 北 精装 NaN   459079 NaN 东 南 西 北 精装 NaN   459329 NaN 东 南 西 北 有电梯 NaN   461873 NaN 东 南 西 北 精装 NaN   462182 NaN 东 南 西 北 简装 NaN   463843 NaN 东 南 西 北 其他 NaN   464149 NaN 东 南 西 北 精装 NaN   464420 NaN 东 南 西 北 毛坯 NaN   466154 NaN 东 南 西 北 简装 NaN   466251 NaN 东 南 西 北 精装 NaN   466384 NaN 东 南 西 北 精装 NaN   466511 NaN 东 南 西 北 精装 NaN   466682 NaN 东 南 西 北 精装 NaN   467575 NaN 东 南 西 北 简装 NaN   467610 NaN 东 南 西 北 精装 NaN   468418 NaN 东 南 西 北 简装 NaN   468430 NaN 东 南 西 北 简装 NaN   468437 NaN 东 南 西 北 精装 NaN   468438 NaN 东 南 西 北 精装 NaN   468472 NaN 东 南 西 北 其他 NaN   468798 NaN 东 南 西 北 精装 NaN   470970 NaN 东 南 西 北 简装 NaN   471830 NaN 东 南 西 北 简装 NaN   471920 NaN 东 南 西 北 精装 NaN   472977 NaN 东 南 西 北 精装 NaN   473544 NaN 东 南 西 北 精装 NaN    130 rows × 4 columns\n dataset['info_flood'].unique()  array(['高楼层(共9层)2008年建板塔结合 - ', '上叠(共4层)2008年建暂无数据 - ', '高楼层(共16层)2013年建板楼 - ', ..., '低楼层(共4层)2009年建塔楼 - ', '中楼层(共38层)2015年建塔楼 - ', '1层2005年建塔楼 - '], dtype=object)  dataset['type_house'].unique()  array([nan, '2层', '顶层(共6层)', '底层(共2层)', '高楼层(共6层)', '中楼层(共17层)', '顶层(共9层)', '低楼层(共16层)', '高楼层(共16层)', '高楼层(共7层)', '低楼层(共3层)', '3层', '中楼层(共5层)', '底层(共9层)', '中楼层(共10层)', '中楼层(共4层)', '中楼层(共6层)', '底层(共3层)', '5层', '底层(共1层)', '顶层(共3层)', '高楼层(共15层)', '中楼层(共9层)', '低楼层(共11层)', '高楼层(共13层)', '底层(共6层)', '底层(共15层)', '底层(共11层)', '低楼层(共17层)', '顶层(共2层)', '顶层(共5层)', '低楼层(共15层)', '顶层(共16层)', '高楼层(共10层)', '低楼层(共9层)', '中楼层(共15层)', '顶层(共14层)', '中楼层(共13层)', '中楼层(共20层)', '低楼层(共18层)', '高楼层(共9层)', '底层(共5层)', '中楼层(共14层)', '底层(共16层)', '低楼层(共13层)', '顶层(共4层)', '低楼层(共10层)', '底层(共4层)', '底层(共18层)', '低楼层(共6层)', '顶层(共8层)', '高楼层(共18层)', '中楼层(共18层)', '顶层(共17层)', '中楼层(共11层)', '底层(共12层)', '高楼层(共12层)', '顶层(共11层)', '高楼层(共17层)', '顶层(共12层)', '中楼层(共12层)', '中楼层(共7层)', '底层(共14层)', '中楼层(共16层)', '高楼层(共14层)', '低楼层(共14层)', '低楼层(共7层)', '顶层(共7层)', '地下室(共6层)', '顶层(共13层)', '低楼层(共12层)', '底层(共13层)', '高楼层(共20层)', '低楼层(共24层)', '4层', '1层', '高楼层(共24层)', '中楼层(共21层)', '中楼层(共24层)', '高楼层(共19层)', '顶层(共21层)', '底层(共19层)', '底层(共21层)', '高楼层(共8层)', '2013年建板楼', '中楼层(共8层)', '中楼层(共25层)', '低楼层(共25层)', '底层(共25层)', '低楼层(共26层)', '底层(共24层)', '顶层(共27层)', '高楼层(共22层)', '6层', '低楼层(共22层)', '中楼层(共28层)', '中楼层(共27层)', '顶层(共23层)', '中楼层(共26层)', '高楼层(共28层)', '低楼层(共27层)', '高楼层(共27层)', '低楼层(共23层)', '底层(共28层)', '底层(共26层)', '顶层(共24层)', '低楼层(共28层)', '中楼层(共23层)', '顶层(共10层)', '高楼层(共23层)', '2013年建暂无数据', '低楼层(共20层)', '底层(共10层)', '高楼层(共25层)', '高楼层(共11层)', '高楼层(共26层)', '低楼层(共21层)', '底层(共20层)', '高楼层(共21层)', '中楼层(共19层)', '2011年建板塔结合', '2015年建板楼', '2012年建板楼', '11层', '2012年建暂无数据', '低楼层(共19层)', '顶层(共18层)', '2014年建板楼', '2012年建板塔结合', '2013年建板塔结合', '中楼层(共29层)', '地下室(共3层)', '顶层(共15层)', '上叠(共5层)', '顶层(共20层)', '9层', '下叠(共5层)', '低楼层(共8层)', '2011年建暂无数据', '顶层(共25层)', '底层(共8层)', '底层(共7层)', '地下室(共15层)', '2013年建塔楼', '地下室(共7层)', '地下室(共10层)', '15层', '下叠(共4层)', '地下室(共2层)', '上叠(共6层)', '地下室(共4层)', '上叠(共4层)', '上叠(共3层)', '底层(共22层)', '底层(共17层)', '地下室(共5层)', '8层', '中楼层(共22层)', '2009年建暂无数据', '低楼层(共36层)', '中楼层(共36层)', '顶层(共22层)', '顶层(共19层)', '7层', '低楼层(共31层)', '中楼层(共31层)', '高楼层(共31层)', '12层', '2012年建塔楼', '底层(共27层)', '2008年建板楼', '地下室(共18层)', '中楼层(共30层)', '18层', '2014年建塔楼', '2014年建暂无数据', '低楼层(共29层)', '2016年建板塔结合', '中楼层(共33层)', '2015年建暂无数据', '高楼层(共29层)', '低楼层(共33层)', '高楼层(共33层)', '低楼层(共34层)', '高楼层(共34层)', '顶层(共34层)', '中楼层(共34层)', '底层(共29层)', '顶层(共28层)', '低楼层(共30层)', '顶层(共26层)', '底层(共30层)', '高楼层(共30层)', '顶层(共29层)', '顶层(共30层)', '地下室(共12层)', '中楼层(共32层)', '高楼层(共32层)', '2007年建板塔结合', '低楼层(共32层)', '顶层(共32层)', '底层(共31层)', '2001年建板楼', '暂无数据', '19层', '2012年建平房', '平房', '2014年建平房', '2016年建暂无数据', '14层', '2008年建板塔结合', '地下室(共8层)', '底层(共32层)', '地下室(共11层)', '2010年建暂无数据', '低楼层(共35层)', '低楼层(共40层)', '顶层(共35层)', '中楼层(共35层)', '高楼层(共40层)', '高楼层(共35层)', '2011年建板楼', '2005年建板楼', '2007年建板楼', '2010年建板楼', '2009年建板楼', '2006年建板楼', '底层(共23层)', '2009年建板塔结合', '2005年建暂无数据', '2006年建暂无数据', '2002年建板塔结合', '2004年建暂无数据', '地下室(共9层)', '2006年建塔楼', '2008年建暂无数据', '地下室(共14层)', '2014年建板塔结合', '2004年建板楼', '2005年建塔楼', '地下室(共16层)', '16层', '低楼层(共63层)', '底层(共33层)', '板楼', '2006年建板塔结合', '2007年建暂无数据', '中楼层(共57层)', '高楼层(共57层)', '中楼层(共42层)', '低楼层(共42层)', '高楼层(共42层)', '中楼层(共63层)', '27层', '板塔结合', '地下室(共0层)', '2010年建塔楼', '2011年建塔楼', '21层', '20层', '25层', '顶层(共33层)', '17层', '2004年建塔楼', '底层(共35层)', '底层(共34层)', '顶层(共31层)', '2009年建塔楼', '2008年建塔楼', '地下室(共21层)', '地下室(共28层)', '地下室(共1层)', '中楼层(共37层)', '28层', '低楼层(共37层)'], dtype=object)  dataset[dataset['type_house']=='地下室(共21层)'][['type_house','years_house']]   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    type_house years_house     469271 地下室(共21层) 1991年建塔楼     dataset['years_house'].unique()  array([nan, '2010年建暂无数据', '2012年建暂无数据', '2011年建暂无数据', '2015年建暂无数据', '2009年建暂无数据', '2006年建暂无数据', '暂无数据', '2016年建暂无数据', '2004年建暂无数据', '2017年建暂无数据', '2014年建暂无数据', '1998年建暂无数据', '2013年建板楼', '2013年建暂无数据', '2011年建板楼', '2005年建板楼', '2011年建塔楼', '板楼', '2008年建暂无数据', '2001年建暂无数据', '2008年建板楼', '1979年建板楼', '2015年建板楼', '2004年建板楼', '1992年建板楼', '1985年建暂无数据', '1997年建板楼', '2002年建板楼', '1996年建暂无数据', '2006年建板楼', '2015年建塔楼', '2012年建板楼', '2003年建暂无数据', '1986年建板楼', '1995年建板楼', '1986年建暂无数据', '2010年建板楼', '1981年建暂无数据', '1993年建板楼', '1989年建板楼', '1995年建板塔结合', '2003年建板楼', '1998年建板楼', '2007年建板楼', '2007年建暂无数据', '1997年建暂无数据', '2014年建板楼', '2016年建板楼', '1999年建板楼', '2001年建板楼', '2000年建板楼', '1996年建板楼', '2005年建板塔结合', '1987年建板楼', '2006年建塔楼', '2006年建板塔结合', '1970年建板楼', '1990年建板楼', '1980年建板楼', '1982年建板楼', '1994年建板楼', '1983年建板楼', '1985年建板楼', '1991年建板楼', '1988年建板楼', '2003年建塔楼', '2009年建板楼', '1981年建板楼', '2017年建板楼', '2016年建板塔结合', '2015年建板塔结合', '1958年建暂无数据', '1975年建板楼', '1984年建板楼', '1966年建板楼', '2013年建板塔结合', '2013年建塔楼', '1988年建塔楼', '1980年建塔楼', '2001年建塔楼', '2010年建板塔结合', '2016年建塔楼', '2012年建塔楼', '2005年建暂无数据', '2017年建塔楼', '2012年建板塔结合', '2009年建板塔结合', '2014年建塔楼', '2014年建板塔结合', '1999年建暂无数据', '2007年建板塔结合', '2008年建板塔结合', '2010年建塔楼', '2011年建板塔结合', '板塔结合', '2007年建塔楼', '2009年建塔楼', '2004年建板塔结合', '2000年建暂无数据', '2008年建塔楼', '1995年建暂无数据', '2003年建板塔结合', '1988年建暂无数据', '2002年建暂无数据', '1992年建暂无数据', '1989年建暂无数据', '2000年建板塔结合', '1994年建塔楼', '1979年建暂无数据', '1980年建暂无数据', '1984年建暂无数据', '2001年建板塔结合', '1994年建暂无数据', '2002年建板塔结合', '1977年建板楼', '1975年建暂无数据', '1976年建板楼', '2004年建塔楼', '1978年建暂无数据', '1976年建暂无数据', '1993年建暂无数据', '2002年建塔楼', '1996年建塔楼', '1986年建塔楼', '1994年建板塔结合', '1991年建板塔结合', '2000年建塔楼', '1999年建塔楼', '1999年建板塔结合', '塔楼', '1997年建塔楼', '1978年建板楼', '2005年建塔楼', '1960年建板楼', '1998年建塔楼', '1995年建塔楼', '1998年建板塔结合', '1962年建板楼', '1996年建板塔结合', '1990年建暂无数据', '1960年建暂无数据', '1982年建暂无数据', '1990年建板塔结合', '1997年建板塔结合', '1992年建塔楼', '1964年建板楼', '1958年建板楼', '1973年建板楼', '1983年建塔楼', '1985年建塔楼', '1987年建塔楼', '1974年建板楼', '1984年建塔楼', '1989年建板塔结合', '1990年建塔楼', '1991年建塔楼', '1987年建暂无数据', '1950年建板楼', '1989年建塔楼', '1993年建塔楼', '1963年建板楼', '1992年建板塔结合', '1988年建板塔结合', '1984年建板塔结合', '1985年建板塔结合', '1993年建板塔结合', '1983年建板塔结合', '1987年建板塔结合', '1986年建板塔结合', '1956年建板楼', '1954年建板楼', '1957年建板楼', '1955年建板楼', '1968年建板楼', '1978年建板塔结合', '1959年建板楼', '1965年建板楼', '1995年建平房', '1980年建板塔结合', '1982年建板塔结合', '1967年建板楼', '1982年建塔楼', '1963年建板塔结合', '1970年建板塔结合', '1981年建塔楼', '1980年建平房', '1961年建板楼', '1950年建暂无数据', '1979年建塔楼', '1972年建板楼', '1981年建板塔结合', '1991年建暂无数据', '1979年建板塔结合', '1971年建板楼', '1952年建板楼', '1953年建板楼', '1969年建板楼', '1977年建板塔结合', '1970年建暂无数据'], dtype=object)  dataset.info()  \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 474301 entries, 0 to 474300 Data columns (total 39 columns): introduction_house 474301 non-null object community_house 474301 non-null object href_house 474301 non-null object unit_house 38137 non-null object size_house 38137 non-null object direction_house 38137 non-null object decoration_house 38109 non-null object elevator_house 37093 non-null object type_house 38137 non-null object years_house 38100 non-null object area_house 474301 non-null object interests_house 38137 non-null object watch_times 38137 non-null object submit_period 38137 non-null object years_period 30543 non-null object tax_free 35260 non-null object total_price 474301 non-null float64 smeter_price 474301 non-null object region 474301 non-null object info_cluster 436164 non-null object info_flood 436164 non-null object info_follow 436164 non-null object years_house_year_edit1 37850 non-null float64 size_house_edit1 474301 non-null float64 size_house_edit1_addcata 474301 non-null object watch_time_edit1 474301 non-null int64 watch_time_edit1_addcata 474301 non-null object interests_house_edit1 474301 non-null int64 interests_house_edit1_addcata 474301 non-null object submit_period_edit1 474301 non-null int64 submit_period_edit1_addcata 474301 non-null object years_period_edit1 474301 non-null object tax_free_edit1 474301 non-null object smeter_price_edit1 474301 non-null int64 direction_edit1 474301 non-null object decoration_edit1 474301 non-null object elevator_edit1 474301 non-null object type_house_edit1 474301 non-null object years_house_type_edit1 474301 non-null object dtypes: float64(3), int64(4), object(32) memory usage: 141.1+ MB  dataset.columns  Index(['introduction_house', 'community_house', 'href_house', 'unit_house', 'size_house', 'direction_house', 'decoration_house', 'elevator_house', 'type_house', 'years_house', 'area_house', 'interests_house', 'watch_times', 'submit_period', 'years_period', 'tax_free', 'total_price', 'smeter_price', 'region', 'info_cluster', 'info_flood', 'info_follow', 'years_house_year_edit1', 'size_house_edit1', 'size_house_edit1_addcata', 'watch_time_edit1', 'watch_time_edit1_addcata', 'interests_house_edit1', 'interests_house_edit1_addcata', 'submit_period_edit1', 'submit_period_edit1_addcata', 'years_period_edit1', 'tax_free_edit1', 'smeter_price_edit1', 'direction_edit1', 'decoration_edit1', 'elevator_edit1', 'type_house_edit1', 'years_house_type_edit1'], dtype='object')  feature1 = ['community_house','unit_house','size_house_edit1','size_house_edit1_addcata','watch_time_edit1','watch_time_edit1_addcata','interests_house_edit1','interests_house_edit1_addcata','submit_period_edit1','submit_period_edit1_addcata','years_period_edit1','tax_free_edit1','total_price','smeter_price_edit1', 'direction_edit1','decoration_edit1','elevator_edit1','type_house_edit1','years_house_type_edit1','years_house_year_edit1','region']  dataset_used1 = dataset[feature1]  dataset['region'].unique()  array(['cd', 'xm', 'sh', 'sz', 'zz', 'qd', 'su', 'cq', 'dl', 'hf', 'sjz', 'cs', 'wh', 'nj', 'hz', 'tj', 'gz', 'bj', 'lf', 'jn', 'fs', 'zh', 'zs'], dtype=object)  dataset_used1.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    community_house unit_house size_house_edit1 size_house_edit1_addcata watch_time_edit1 watch_time_edit1_addcata interests_house_edit1 interests_house_edit1_addcata submit_period_edit1 submit_period_edit1_addcata years_period_edit1 tax_free_edit1 total_price smeter_price_edit1 direction_edit1 decoration_edit1 elevator_edit1 type_house_edit1 years_house_type_edit1 years_house_year_edit1 region     0 麓山国际帕萨迪纳3组 NaN 98.070 0 -1 1 -1 1 -1 2 0 0 250.000 25492 nodata nodata 有电梯 nodata nodata nan cd   1 麓山国际塞尔维蒙 NaN 206.000 0 -1 1 -1 1 -1 2 0 0 420.000 20389 nodata nodata 无电梯 nodata nodata nan cd   2 麓山国际半月湾 NaN 112.190 0 -1 1 -1 1 -1 2 0 0 275.000 24512 nodata 其他 nodata nodata nodata nan cd   3 心怡中丝园 NaN 87.560 0 -1 1 -1 1 -1 2 0 0 193.000 22043 nodata nodata 有电梯 nodata nodata nan cd   4 麓山国际茵特拉肯A NaN 128.740 0 -1 1 -1 1 -1 2 0 0 300.000 23303 nodata 其他 nodata nodata nodata nan cd     dataset_used1.info()  \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 474301 entries, 0 to 474300 Data columns (total 21 columns): community_house 474301 non-null object unit_house 38137 non-null object size_house_edit1 474301 non-null float64 size_house_edit1_addcata 474301 non-null object watch_time_edit1 474301 non-null int64 watch_time_edit1_addcata 474301 non-null object interests_house_edit1 474301 non-null int64 interests_house_edit1_addcata 474301 non-null object submit_period_edit1 474301 non-null int64 submit_period_edit1_addcata 474301 non-null object years_period_edit1 474301 non-null object tax_free_edit1 474301 non-null object total_price 474301 non-null float64 smeter_price_edit1 474301 non-null int64 direction_edit1 474301 non-null object decoration_edit1 474301 non-null object elevator_edit1 474301 non-null object type_house_edit1 474301 non-null object years_house_type_edit1 474301 non-null object years_house_year_edit1 37850 non-null float64 region 474301 non-null object dtypes: float64(3), int64(4), object(14) memory usage: 76.0+ MB  绘图分析 # # 绘图 # fig, ax = plt.subplots() # ax.scatter(x = dataset['GrLivArea'], y = dataset['total_price']) # plt.ylabel('total_price', fontsize=13) # plt.xlabel('GrLivArea', fontsize=13) # plt.show()  单变量观察 yjiu = ['community_house','unit_house','size_house_edit1','size_house_edit1_addcata','watch_time_edit1','watch_time_edit1_addcata','interests_house_edit1','interests_house_edit1_addcata','submit_period_edit1','submit_period_edit1_addcata','years_period_edit1','tax_free_edit1','total_price','smeter_price_edit1','direction_edit1','decoration_edit1','elevator_edit1','type_house_edit1','years_house_type_edit1','years_house_year_edit1']  print(yjiu)  ['community_house', 'unit_house', 'size_house_edit1', 'size_house_edit1_addcata', 'watch_time_edit1', 'watch_time_edit1_addcata', 'interests_house_edit1', 'interests_house_edit1_addcata', 'submit_period_edit1', 'submit_period_edit1_addcata', 'years_period_edit1', 'tax_free_edit1', 'total_price', 'smeter_price_edit1', 'direction_edit1', 'decoration_edit1', 'elevator_edit1', 'type_house_edit1', 'years_house_type_edit1', 'years_house_year_edit1']  面积、查看次数、收藏次数、发布时间 房屋面积：size_house_edit1_addcata 查看次数：watch_time_edit1_addcata 感兴趣人数：interests_house_edit1_addcata 多久前发布：submit_period_edit1_addcata\nfig1 = plt.figure(figsize=(10,10)) fig1.add_subplot(221) fg1 = sns.distplot(dataset_used1[dataset_used1['size_house_edit1_addcata']=='0']['size_house_edit1'],bins=1000,kde=False,color='b') fg1.set(xlim=(0,500)) plt.title('size') fig1.add_subplot(222) fg2 = sns.distplot(dataset_used1[dataset_used1['watch_time_edit1_addcata']=='0']['watch_time_edit1'],bins=500,kde=False,color='b') fg2.set(xlim=(0,50)) plt.title('watch_time') fig1.add_subplot(223) fg3 = sns.distplot(dataset_used1[dataset_used1['interests_house_edit1_addcata']=='0']['interests_house_edit1'],bins=1000,kde=False,color='b') fg3.set(xlim=(0,500)) plt.title('interests_house') fig1.add_subplot(224) fg4 = sns.distplot(dataset_used1[dataset_used1['submit_period_edit1_addcata']=='0']['submit_period_edit1'],bins=200,kde=False,color='b') fg4.set(xlim=(0,100)) plt.title('submit_period_edit1')  Text(0.5,1,'submit_period_edit1')  从上面的结果观察，size的分布呈现偏右整体分布；watch_times和interest_house呈现由小到大下降的分布；submit_period呈现非均匀分布状态，发现＞30天以上的数量明显要多很多。\n2年产权、5年产权、房屋户型 产权是满足5年：tax_free_edit1\n产权是否满2年：years_period_edit1\n房屋户型：unit_house\nfig2_1 = plt.figure(figsize=(10,6)) ax3 = fig2_1.add_subplot(221) dataset_used1['tax_free_edit1'].value_counts().plot(kind='bar') plt.title('tax_free') ax2 = fig2_1.add_subplot(222) dataset_used1['years_period_edit1'].value_counts().plot(kind='bar') plt.title('years_period') ax1 = fig2_1.add_subplot(212) ax1.margins(0.05) # Default margin is 0.05, value 0 means fit dataset_used1['unit_house'].value_counts().plot(kind='bar') plt.title('unit_house')  Text(0.5,1,'unit_house')  观察到2年产权大多数不满足，5年产权的情况类似；房屋类型主要以2室1厅、3室1厅、1室1厅，3室2厅、2室2厅、4室2厅为主。\ndataset_used1['unit_house'].value_counts().head(10)  2室1厅 13889 3室1厅 6080 1室1厅 4770 3室2厅 4339 2室2厅 2524 4室2厅 1670 1室0厅 1105 4室1厅 492 2房间1卫 443 1房间1卫 438 Name: unit_house, dtype: int64  从上面结果观察到，2年、5年产权中，没有产权的占比最大，户型类别当中，集中在前十种类型，其中2室1厅占主要\nfig2 = plt.figure(figsize=(6,6)) fig2.add_subplot(111) fg1 = sns.distplot(dataset_used1['total_price'],bins=1500,kde=False,color='b') fg1.set(xlim=(0,2000)) plt.title('total_price')  Text(0.5,1,'total_price')  从总价钱的分布当中观察，total_price呈现偏右正态分布。\n朝向、装修程度、电梯配备、楼层位置、楼型、建成时间 'direction_edit1', 'decoration_edit1', 'elevator_edit1', 'type_house_edit1', 'years_house_type_edit1', 'years_house_year_edit1'  ('direction_edit1', 'decoration_edit1', 'elevator_edit1', 'type_house_edit1', 'years_house_type_edit1', 'years_house_year_edit1')  # 朝向类别——过多不好绘图 dataset_used1['direction_edit1'].unique()  array(['nodata', ' 东 南 西 北', ' 南', ' 南 北', ' 北', ' 西南', ' 东南', ' 东 南', ' 东', ' 西北', ' 西', ' 南 西', ' 东北', ' 东南 西北', ' 东 东南', '南 北', '东南', '东 南 西 北', '东 东南 南 西南 西', '东 南 北', '南', '东 西', '东南 西北', '东 南', '西', '东 西 北', '东北', '东南 南 北', '南 西 北', '西北', '东 南 西', '东 北', '北', '西南', '东南 北', '东', '南 西', '西 北', '东南 西南', '西南 北', '南 西北', '东南 西', '东 西北', '南 西南', '南 西南 北', '南 西北 北', '东 北 东北', '东 西 东北', '南 北 东北', '东 东南 南 北', '东南 西 北', '西南 东北', '东南 东北', '东 东南', '南 北 西', '东南 南', '东南 南 西南', '西 东北', '南 东', '北 东北', '南 北 东', '西北 北', '南 东北', '西南 西', '南 东 北', '东南 西南 西北', '东 西南', '西 西北', '西 西北 北', ' 东 西', '南 西 东北', '东 西北 北', '南 西南 东北', '西 东', '北 西南', '西南 西北', '东 东南 西北 北', '东南 南 东北', '东南 南 西北', '南 西 西北', '东 西 北 东北', '南 西南 西 西北 东北', '西南 西北 东北', '西北 北 东北', '东南 西南 北', '东 南 西北', '西 西南', '东 东北', '东南 西北 北', '西北 东北', '北 西', '东 西 西北', '北 东', '东 东南 南', '北 南', '东南 西南 东北', '西南 西 北', '东 西南 东北', '东南 西北 东北', '东 西南 西', '南 西南 西', '东 西南 北'], dtype=object)  # 装饰 dataset_used1['decoration_edit1'].unique()  array(['nodata', '其他', '毛坯', '精装', '简装'], dtype=object)  dataset_used1['elevator_edit1'].unique()  array(['有电梯', '无电梯', 'nodata'], dtype=object)  dataset_used1['type_house_edit1'].unique()  array(['nodata', '低楼层', '顶层', '底层', '高楼层', '中楼层', '地下室', '上叠', '下叠'], dtype=object)  dataset_used1['years_house_type_edit1'].unique()  array(['nodata', '板', '塔', '板塔', '别墅', '平房'], dtype=object)  fig3 = plt.figure(figsize=(10,10)) fig3.add_subplot(221) dataset_used1['decoration_edit1'].value_counts().plot(kind='bar') plt.title('decoration_edit1') fig3.add_subplot(222) dataset_used1['elevator_edit1'].value_counts().plot(kind='bar') plt.title('elevator_edit1') fig3.add_subplot(223) dataset_used1['type_house_edit1'].value_counts().plot(kind='bar') plt.title('type_house_edit1') fig3.add_subplot(224) dataset_used1['years_house_type_edit1'].value_counts().plot(kind='bar') plt.title('years_house_type_edit1') plt.rcParams['font.sans-serif'] = ['SimHei']  对有数据的情况观察：\n装修以精装\u0026gt;简装\u0026gt;毛坯；\n电梯：有\u0026gt;无 楼层前3：中楼层\u0026gt;低楼层\u0026gt;高楼层 楼类型前3：板\u0026gt;塔\u0026gt;板塔\nfig31 = plt.figure(figsize=(10,6)) fig31.add_subplot(111) dataset_used1['years_house_year_edit1'].value_counts().plot(kind='bar') plt.title('years_house_year_edit1')  Text(0.5,1,'years_house_year_edit1')  从房屋建造年代看，以2010年建造的为主，其中2003-2005三年最多。\n多维度分析 产权和查看次数、收藏次数 2年产权的查看次数、收藏次数 5年产权的产看次数、收藏次数 2室1厅户型的具有2年产权查看次数 2室1厅户型的具有5年产权查看次数\n产权是满足5年：tax_free_edit1 产权是否满2年：years_period_edit1 房屋户型：unit_house 收藏次数watch_time_edit1 感兴趣人数：interests_house_edit1\ncond_dataset1 = dataset_used1[dataset_used1['years_period_edit1']=='0']['watch_time_edit1'].value_counts() fig42 = plt.figure(figsize=(12,12)) fig42.add_subplot(221) fg1 = sns.distplot(cond_dataset1[1:],bins=200,kde=False,color='b') fg1.set(xlim=(0,400)) plt.title('产权是满2年的收藏次数') cond_dataset2 = dataset_used1[dataset_used1['years_period_edit1']=='0']['interests_house_edit1'].value_counts() fig42.add_subplot(222) fg1 = sns.distplot(cond_dataset2[1:],bins=200,kde=False,color='b') fg1.set(xlim=(0,200)) plt.title('产权是满2年的感兴趣人数') cond_dataset3 = dataset_used1[dataset_used1['tax_free_edit1']=='0']['watch_time_edit1'].value_counts() fig42.add_subplot(223) fg1 = sns.distplot(cond_dataset1[1:],bins=200,kde=False,color='b') fg1.set(xlim=(0,400)) plt.title('产权是满5年的收藏次数') cond_dataset4 = dataset_used1[dataset_used1['tax_free_edit1']=='0']['interests_house_edit1'].value_counts() fig42.add_subplot(224) fg1 = sns.distplot(cond_dataset2[1:],bins=200,kde=False,color='b') fg1.set(xlim=(0,200)) plt.title('产权是满5年的感兴趣人数')  Text(0.5,1,'产权是满5年的感兴趣人数')  从上面结果来看，对于产权在2年和5年，收藏次数和关注人数分布以10次以下群体为主。呈现明显偏右分布。\n户型+产权和查看次数、收藏次数¶ cond_dataset1 = dataset_used1[(dataset_used1['unit_house']=='2室1厅')\u0026amp;(dataset_used1['years_period_edit1']=='0')]['watch_time_edit1'].value_counts() fig42 = plt.figure(figsize=(16,8)) fig42.add_subplot(121) fg1 = sns.barplot(x=cond_dataset1[1:].index, y=cond_dataset1[1:].values) # fg1.set(xlim=(0,400)) plt.title('产权是满2年的收藏次数') cond_dataset2 = dataset_used1[(dataset_used1['unit_house']=='2室1厅')\u0026amp;(dataset_used1['years_period_edit1']=='0')]['interests_house_edit1'].value_counts() fig42.add_subplot(122) fg1 = sns.barplot(x=cond_dataset2[1:].index, y=cond_dataset2[1:].values) # fg1.set(xlim=(0,200)) plt.title('产权是满2年的感兴趣人数')  Text(0.5,1,'产权是满2年的感兴趣人数')  从多条件看，在2室一厅的房型中，2年产权的收藏次数和关注人数均呈现逐渐偏右分布。\ndataset_used1['unit_house'].unique()  array([nan, '4室2厅', '3室2厅', '5室3厅', '2室1厅', '6室2厅', '9室9厅', '2室2厅', '4室1厅', '3室1厅', '5室2厅', '1室1厅', '3室3厅', '7室4厅', '4室3厅', '6室4厅', '7室3厅', '7室2厅', '5室4厅', '5室1厅', '1室0厅', '2室0厅', '1室2厅', '车位', '3房间1卫', '6房间2卫', '联排别墅', '2房间1卫', '1房间1卫', '5房间2卫', '2房间2卫', '2房间0卫', '1房间0卫', '6室3厅', '3房间0卫', '3房间2卫', '叠拼别墅', '4房间1卫', '4室4厅', '4房间2卫', '3室0厅', '4室0厅', '独栋别墅', '8室3厅', '双拼别墅', '8室2厅', '2室3厅', '9室2厅', '4室5厅', '5房间3卫', '4房间3卫', '11房间3卫', '6房间4卫', '6房间3卫', '9室1厅', '5房间1卫', '6室1厅', '8室4厅', '9房间3卫', '9室3厅', '5房间0卫', '6室0厅', '7房间2卫', '5室0厅', '8室5厅', '9室4厅', '7室5厅', '3房间3卫', '1房间2卫', '8房间4卫', '8房间3卫', '7室0厅', '5室5厅', '7室1厅', '6室5厅', '6室6厅', '6房间5卫'], dtype=object)  cond = (dataset_used1['years_period_edit1']=='1')\u0026amp;(dataset_used1['unit_house']=='2室1厅') dataset_used1[cond][['interests_house_edit1','watch_time_edit1']].head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    interests_house_edit1 watch_time_edit1     337734 32 0   342189 103 8   342194 55 2   342207 38 4   342210 1 0     dataset_used1['years_house_type_edit1'].unique()  array(['nodata', '板', '塔', '板塔', '别墅', '平房'], dtype=object)  dataset_used1.ix[337734,:]  community_house 龙湖长城源著 unit_house 2室1厅 size_house_edit1 0.000 size_house_edit1_addcata 1 watch_time_edit1 0 watch_time_edit1_addcata 0 interests_house_edit1 32 interests_house_edit1_addcata 0 submit_period_edit1 29 submit_period_edit1_addcata 0 years_period_edit1 1 tax_free_edit1 1 total_price 235.000 smeter_price_edit1 28266 direction_edit1 东南 decoration_edit1 其他 elevator_edit1 nodata type_house_edit1 低楼层 years_house_type_edit1 nodata years_house_year_edit1 2015.000 region bj Name: 337734, dtype: object  dataset_used1['region'].unique()  array(['cd', 'xm', 'sh', 'sz', 'zz', 'qd', 'su', 'cq', 'dl', 'hf', 'sjz', 'cs', 'wh', 'nj', 'hz', 'tj', 'gz', 'bj', 'lf', 'jn', 'fs', 'zh', 'zs'], dtype=object)  关联分析 目标变量，房子单价，smeter_price_edit1。\n不同的面积-单价，size_house_edit1\n不同地区-单价，region\n2年产权-单价，years_period_edit1\n5年产权-单价，tax_free_edit1\n不同户型-单价，unit_house\n** 不同查看次数-单价，watch_time_edit1**\n** 不同收藏次数-单价，interests_house_edit1**\n不同装修程度-单价，decoration_edit1\n** 不同朝向-单价，direction_edit1**\n不同电梯配置-单价，elevator_edit1\n不同楼层位置-单价，type_house_edit1\n不同楼型-单价,years_house_type_edit1\n不同建成时间-单价,years_house_year_edit1\n** 不同小区-单价**\ndataset_used1.groupby('years_period_edit1')['smeter_price_edit1'].mean()  years_period_edit1 0 31620.411 1 64924.182 Name: smeter_price_edit1, dtype: float64  2年产权、5年产权vs装修程度、户型 fig5 = plt.figure(figsize=(10,6)) fig5.suptitle('2年产权、5年产权、装修程度、户型') fig5.add_subplot(231) dataset_used1.groupby('years_period_edit1')['smeter_price_edit1'].mean().plot(kind='bar') plt.title('2年产权') fig5.add_subplot(232) dataset_used1.groupby('tax_free_edit1')['smeter_price_edit1'].mean().plot(kind='bar') plt.title('5年产权') fig5.add_subplot(233) dataset_used1.groupby('decoration_edit1')['smeter_price_edit1'].mean().plot(kind='bar') plt.title('装修程度') fig5.add_subplot(212) dataset_used1.groupby('unit_house')['smeter_price_edit1'].mean().plot(kind='bar') plt.title('不同户型')  Text(0.5,1,'不同户型')  电梯、楼层、楼型、建成时间单变量统计 fig5 = plt.figure(figsize=(10,6)) # plt.tight_layout(pad=1) fig5.suptitle('电梯、楼层、楼型、建成时间') plt.subplots_adjust(wspace=0.5,hspace=0.5) fig5.add_subplot(231) dataset_used1.groupby('elevator_edit1')['smeter_price_edit1'].mean().plot(kind='bar') plt.title('电梯') fig5.add_subplot(232) dataset_used1.groupby('type_house_edit1')['smeter_price_edit1'].mean().plot(kind='bar') plt.title('楼层') fig5.add_subplot(233) dataset_used1.groupby('years_house_type_edit1')['smeter_price_edit1'].mean().plot(kind='bar') plt.title('楼型') fig5.add_subplot(212) dataset_used1.groupby('years_house_year_edit1')['smeter_price_edit1'].mean().plot(kind='bar') plt.title('建成时间')  Text(0.5,1,'建成时间')  连续变量的相关性 dataset_used1.corr()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    size_house_edit1 watch_time_edit1 interests_house_edit1 submit_period_edit1 total_price smeter_price_edit1 years_house_year_edit1     size_house_edit1 1.000 -0.193 -0.225 -0.269 0.373 -0.087 nan   watch_time_edit1 -0.193 1.000 0.617 0.544 0.147 0.225 -0.017   interests_house_edit1 -0.225 0.617 1.000 0.651 0.138 0.235 -0.118   submit_period_edit1 -0.269 0.544 0.651 1.000 0.217 0.287 0.026   total_price 0.373 0.147 0.138 0.217 1.000 0.635 0.134   smeter_price_edit1 -0.087 0.225 0.235 0.287 0.635 1.000 -0.250   years_house_year_edit1 nan -0.017 -0.118 0.026 0.134 -0.250 1.000     plt.subplots(figsize=(8,8)) sns.heatmap(dataset_used1.corr(), vmax=1.0, square=True)  \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x41616358\u0026gt;  异常值检查（size_house_edit1与smeter_price_edit1关系为例） 检查 fig, ax = plt.subplots() ax.scatter(x = dataset_used1.ix[:, 'size_house_edit1'], y = dataset_used1.ix[:, 'smeter_price_edit1']) plt.ylabel('SalePrice', fontsize=13) plt.xlabel('GrLivArea', fontsize=13) plt.show()   从上面的散点图可看出：右下方的几个数据，living area特别大，但是价格又低的离谱，应该是远离市区的无人地带。对最后的分类结果没有影响的离群点（Oultliers），我们可以放心将其删除。  剔除异常点 # 删除离群点 dataset_used1 = dataset_used1.drop(dataset_used1[(dataset_used1['size_house_edit1']\u0026gt;1900) \u0026amp; (dataset_used1['smeter_price_edit1']\u0026lt;1250000)].index) fig, ax = plt.subplots() ax.scatter(x = dataset_used1.ix[:, 'size_house_edit1'], y = dataset_used1.ix[:, 'smeter_price_edit1']) plt.ylabel('SalePrice', fontsize=13) plt.xlabel('GrLivArea', fontsize=13) plt.show()  目标变量处理——满足整体分布  目标值处理：  线性的模型需要正态分布的目标值才能发挥最大的作用。我们需要检测房价什么时候偏离正态分布。使用probplot函数，即正态概率图：   绘制正态分布图 fig5 = plt.figure(figsize=(6,6)) sns.distplot(dataset_used1['smeter_price_edit1'] , fit=norm) # 正态分布拟合 (mu, sigma) = norm.fit(dataset_used1['smeter_price_edit1']) print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))   mu = 33765.16 and sigma = 22811.23  绘制QQ图 看是否与理论的一致\nfig5 = plt.figure(figsize=(6,6)) # 绘图 plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best') plt.ylabel('Frequency') plt.title('SalePrice distribution') # 原始数据分布绘图 res = stats.probplot(dataset_used1['smeter_price_edit1'], plot=plt) plt.show()  从上图结果观察到，与理论分布偏离较大，需要对其进行转换处理，常见的方法可考虑log变换\n变换处理与查看 # 使用log1p函数完成log(1+x)变换 dataset_used1['smeter_price_edit1'] = np.log1p(dataset_used1['smeter_price_edit1'])  C:\\ProgramData\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log1p  # sns.distplot(dataset_used1['smeter_price_edit1'] , fit=norm) # 正态分布拟合 (mu, sigma) = norm.fit(dataset_used1['smeter_price_edit1'])  fig6 = plt.figure(figsize=(6,6)) # 绘图 plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best') plt.ylabel('Frequency') plt.title('SalePrice distribution') # log变换之后的数据分布绘图 res = stats.probplot(dataset_used1['smeter_price_edit1'], plot=plt) plt.show()  C:\\ProgramData\\Miniconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:2320: RuntimeWarning: invalid value encountered in subtract X -= avg[:, None] C:\\ProgramData\\Miniconda3\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in greater return (self.a \u0026lt; x) \u0026amp; (x \u0026lt; self.b) C:\\ProgramData\\Miniconda3\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in less return (self.a \u0026lt; x) \u0026amp; (x \u0026lt; self.b) C:\\ProgramData\\Miniconda3\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:1821: RuntimeWarning: invalid value encountered in less_equal cond2 = cond0 \u0026amp; (x \u0026lt;= self.a)  缺失值处理 dataset_used1.info()  \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; Int64Index: 474294 entries, 0 to 474300 Data columns (total 21 columns): community_house 474294 non-null object unit_house 38137 non-null object size_house_edit1 474294 non-null float64 size_house_edit1_addcata 474294 non-null object watch_time_edit1 474294 non-null int64 watch_time_edit1_addcata 474294 non-null object interests_house_edit1 474294 non-null int64 interests_house_edit1_addcata 474294 non-null object submit_period_edit1 474294 non-null int64 submit_period_edit1_addcata 474294 non-null object years_period_edit1 474294 non-null object tax_free_edit1 474294 non-null object total_price 474294 non-null float64 smeter_price_edit1 474294 non-null float64 direction_edit1 474294 non-null object decoration_edit1 474294 non-null object elevator_edit1 474294 non-null object type_house_edit1 474294 non-null object years_house_type_edit1 474294 non-null object years_house_year_edit1 37850 non-null float64 region 474294 non-null object dtypes: float64(4), int64(3), object(14) memory usage: 79.6+ MB  # temp1_1 =dataset_used1.dtypes # temp2_1 = temp1[temp1=='object'].index # temp3_1 = dataset_used1[temp2_1]==-1' all_data_na3 = (dataset_used1[dataset_used1==-1].sum() / len(dataset_used1)) * 100 all_data_na3 = all_data_na3.drop(all_data_na3[all_data_na3 == 0].index).sort_values(ascending=False)[:30] missing_data3 = pd.DataFrame({'Missing Ratio' :all_data_na3})  f, ax = plt.subplots(figsize=(8, 8)) plt.xticks(rotation='90') sns.barplot(x=all_data_na3.index, y=all_data_na3) plt.xlabel('Features', fontsize=15) plt.ylabel('Percent of missing values', fontsize=15) plt.title('Percent missing data by feature', fontsize=15)  Text(0.5,1,'Percent missing data by feature')  temp1 =dataset_used1.dtypes temp2 = temp1[temp1=='object'].index temp3 = dataset_used1[temp2]=='nodata' all_data_na2 = (temp3.sum() / len(dataset_used1[temp2])) * 100 all_data_na2 = all_data_na2.drop(all_data_na2[all_data_na2 == 0].index).sort_values(ascending=False)[:30] missing_data2 = pd.DataFrame({'Missing Ratio' :all_data_na2})  f, ax = plt.subplots(figsize=(8, 8)) plt.xticks(rotation='90') sns.barplot(x=all_data_na2.index, y=all_data_na2) plt.xlabel('Features', fontsize=15) plt.ylabel('Percent of missing values', fontsize=15) plt.title('Percent missing data by feature', fontsize=15)  Text(0.5,1,'Percent missing data by feature')  all_data_na = (dataset_used1.isnull().sum() / len(dataset_used1)) * 100 all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30] missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})  f, ax = plt.subplots(figsize=(8, 8)) plt.xticks(rotation='90') sns.barplot(x=all_data_na.index, y=all_data_na) plt.xlabel('Features', fontsize=15) plt.ylabel('Percent of missing values', fontsize=15) plt.title('Percent missing data by feature', fontsize=15)  Text(0.5,1,'Percent missing data by feature')  由上面可知道，主要包含有nodata的数据的变量如下，\nunit_house\nyears_house_year_edit1,num\nyears_house_type_edit1\ntype_house_edit1\ndirection_edit1\ndecoration_edit1\nelevator_edit1\nwatch_time_edit1,num\nsubmit_period_edit1,num\ninterests_house_edit1,num\ndataset_used1['unit_house'] = dataset_used1['unit_house'].fillna('nodata') dataset_used1['years_house_year_edit1'] = dataset_used1['years_house_year_edit1'].fillna(0)  # 关于年份的处理，将其处理成距今2018的连续型年值 dataset_used1['years_house_year_edit2'] = dataset_used1['years_house_year_edit1'].apply(lambda x: 0.0 if x==0.0 else (2018.0-x)) # dataset_used1[dataset_used1['years_house_year_edit2']==2018.0] = 0.0  　数值型的缺失，比例达到60%以上，难以通过相关关系的方式来插补缺失值；类别型的数据缺失比列也很大，达到40%以上，先不考虑做处理，而是先将缺失归为一类。 因此，之前在函数当中所做的处理，已经达到了缺失值的处理方式。另外，仅需将数值型的缺失值之前是记录为-1，看后续需要是否调整成0或者其它数值。\ndataset_used1.info()  \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; Int64Index: 474294 entries, 0 to 474300 Data columns (total 22 columns): community_house 474294 non-null object unit_house 474294 non-null object size_house_edit1 474294 non-null float64 size_house_edit1_addcata 474294 non-null object watch_time_edit1 474294 non-null int64 watch_time_edit1_addcata 474294 non-null object interests_house_edit1 474294 non-null int64 interests_house_edit1_addcata 474294 non-null object submit_period_edit1 474294 non-null int64 submit_period_edit1_addcata 474294 non-null object years_period_edit1 474294 non-null object tax_free_edit1 474294 non-null object total_price 474294 non-null float64 smeter_price_edit1 474294 non-null float64 direction_edit1 474294 non-null object decoration_edit1 474294 non-null object elevator_edit1 474294 non-null object type_house_edit1 474294 non-null object years_house_type_edit1 474294 non-null object years_house_year_edit1 474294 non-null float64 region 474294 non-null object years_house_year_edit2 474294 non-null float64 dtypes: float64(5), int64(3), object(14) memory usage: 83.2+ MB  dataset_used1['years_house_year_edit2'].unique()  array([ 0., 8., 6., 7., 3., 9., 12., 2., 14., 1., 4., 20., 5., 13., 10., 17., 39., 26., 33., 21., 16., 22., 15., 32., 23., 37., 25., 29., 11., 19., 18., 31., 48., 28., 38., 36., 24., 35., 27., 30., 60., 43., 34., 52., 41., 42., 40., 58., 56., 54., 45., 44., 68., 55., 62., 64., 61., 63., 50., 59., 53., 51., 57., 46., 47., 66., 65., 49.])  dataset_used1.head(5)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    community_house unit_house size_house_edit1 size_house_edit1_addcata watch_time_edit1 watch_time_edit1_addcata interests_house_edit1 interests_house_edit1_addcata submit_period_edit1 submit_period_edit1_addcata years_period_edit1 tax_free_edit1 total_price smeter_price_edit1 direction_edit1 decoration_edit1 elevator_edit1 type_house_edit1 years_house_type_edit1 years_house_year_edit1 region years_house_year_edit2     0 麓山国际帕萨迪纳3组 nodata 98.070 0 -1 1 -1 1 -1 2 0 0 250.000 10.146 nodata nodata 有电梯 nodata nodata 0.000 cd 0.000   1 麓山国际塞尔维蒙 nodata 206.000 0 -1 1 -1 1 -1 2 0 0 420.000 9.923 nodata nodata 无电梯 nodata nodata 0.000 cd 0.000   2 麓山国际半月湾 nodata 112.190 0 -1 1 -1 1 -1 2 0 0 275.000 10.107 nodata 其他 nodata nodata nodata 0.000 cd 0.000   3 心怡中丝园 nodata 87.560 0 -1 1 -1 1 -1 2 0 0 193.000 10.001 nodata nodata 有电梯 nodata nodata 0.000 cd 0.000   4 麓山国际茵特拉肯A nodata 128.740 0 -1 1 -1 1 -1 2 0 0 300.000 10.056 nodata 其他 nodata nodata nodata 0.000 cd 0.000     其它特征工程 1、有许多特征实际上是类别型的特征，但给出来的是数字，所以需要将其转换成类别型。 # 年份 dataset_used1['years_house_year_edit1'] = dataset_used1['years_house_year_edit1'].astype(int).apply(str)  dataset_used1['region'].unique()  array(['cd', 'xm', 'sh', 'sz', 'zz', 'qd', 'su', 'cq', 'dl', 'hf', 'sjz', 'cs', 'wh', 'nj', 'hz', 'tj', 'gz', 'bj', 'lf', 'jn', 'fs', 'zh', 'zs'], dtype=object)  2、接下来 LabelEncoder，对部分类别的特征进行编号。 temp_ds = dataset_used1  from sklearn.preprocessing import LabelEncoder  temp2  Index(['community_house', 'unit_house', 'size_house_edit1_addcata', 'watch_time_edit1_addcata', 'interests_house_edit1_addcata', 'submit_period_edit1_addcata', 'years_period_edit1', 'tax_free_edit1', 'direction_edit1', 'decoration_edit1', 'elevator_edit1', 'type_house_edit1', 'years_house_type_edit1', 'region'], dtype='object')  temp1 =dataset_used1.dtypes temp2 = temp1[temp1=='object'].index # 使用LabelEncoder做变换 for c in temp2: lbl = LabelEncoder() lbl.fit(list(temp_ds[c].unique())) temp_ds[c] = lbl.transform(list(temp_ds[c].values))  list(temp_ds[c].unique())  [1, 19, 13, 16, 22, 12, 15, 2, 4, 7, 14, 3, 18, 11, 8, 17, 6, 0, 10, 9, 5, 20, 21]  # 查看维度 print('temp_ds的数据维度: {}'.format(temp_ds.shape)) temp_ds.tail()  temp_ds的数据维度: (474294, 22)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    community_house unit_house size_house_edit1 size_house_edit1_addcata watch_time_edit1 watch_time_edit1_addcata interests_house_edit1 interests_house_edit1_addcata submit_period_edit1 submit_period_edit1_addcata years_period_edit1 tax_free_edit1 total_price smeter_price_edit1 direction_edit1 decoration_edit1 elevator_edit1 type_house_edit1 years_house_type_edit1 years_house_year_edit1 region years_house_year_edit2     474296 29485 71 0.000 1 -1 1 -1 1 -1 1 0 0 106.000 8.893 15 0 0 0 0 0 21 0.000   474297 29485 71 0.000 1 -1 1 -1 1 -1 1 0 0 100.000 8.805 15 0 0 0 0 0 21 0.000   474298 19899 71 0.000 1 -1 1 -1 1 -1 1 0 0 18.000 8.923 15 0 0 0 0 0 21 0.000   474299 19899 71 0.000 1 -1 1 -1 1 -1 1 0 0 15.000 8.661 15 0 0 0 0 0 21 0.000   474300 19899 71 0.000 1 -1 1 -1 1 -1 1 0 0 19.000 8.977 15 0 0 0 0 0 21 0.000     temp_ds.describe()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    community_house unit_house size_house_edit1 size_house_edit1_addcata watch_time_edit1 watch_time_edit1_addcata interests_house_edit1 interests_house_edit1_addcata submit_period_edit1 submit_period_edit1_addcata years_period_edit1 tax_free_edit1 total_price smeter_price_edit1 direction_edit1 decoration_edit1 elevator_edit1 type_house_edit1 years_house_type_edit1 years_house_year_edit1 region years_house_year_edit2     count 474294.000 474294.000 474294.000 474294.000 474294.000 474294.000 474294.000 474294.000 474294.000 474294.000 474294.000 474294.000 474294.000 474294.000 474294.000 474294.000 474294.000 474294.000 474294.000 474294.000 474294.000 474294.000   mean 27267.773 66.229 82.180 0.142 0.453 0.920 3.421 0.920 7.452 0.920 0.064 0.074 329.748 -inf 18.814 0.906 0.969 0.410 0.282 4.111 10.437 1.316   std 16331.565 16.362 59.727 0.349 9.743 0.272 25.412 0.272 40.624 0.272 0.245 0.262 360.473 nan 14.090 1.385 0.858 1.502 1.022 14.184 6.538 5.132   min 0.000 0.000 0.000 0.000 -1.000 0.000 -1.000 0.000 -1.000 0.000 0.000 0.000 4.000 -inf 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   25% 12779.000 71.000 50.080 0.000 -1.000 1.000 -1.000 1.000 -1.000 1.000 0.000 0.000 143.000 9.755 15.000 0.000 0.000 0.000 0.000 0.000 4.000 0.000   50% 27253.000 71.000 80.170 0.000 -1.000 1.000 -1.000 1.000 -1.000 1.000 0.000 0.000 235.000 10.153 15.000 0.000 1.000 0.000 0.000 0.000 12.000 0.000   75% 41835.000 71.000 106.100 0.000 -1.000 1.000 -1.000 1.000 -1.000 1.000 0.000 0.000 390.000 10.713 15.000 1.000 2.000 0.000 0.000 0.000 16.000 0.000   max 55144.000 76.000 1823.970 1.000 851.000 1.000 2701.000 1.000 365.000 1.000 1.000 1.000 35000.000 12.206 101.000 4.000 2.000 8.000 5.000 67.000 22.000 68.000     temp_ds['smeter_price_edit1'].unique()  array([10.14615918, 9.92279986, 10.10695887, ..., 11.19759809, 11.28816811, 11.33664164])  3、检查变量的正态分布情况 对房价进行分析，不符合正态分布的将其log转换，使其符合正态分布；那么偏离正态分布太多的特征我们也对它进行转化：\n检查 total_price = temp_ds['total_price'] temp_ds.drop('total_price', axis=1, inplace=True)  # numeric_feats = dataset_used1.dtypes[dataset_used1.dtypes != \u0026quot;object\u0026quot;].index numeric_feats = ['size_house_edit1', 'watch_time_edit1', 'interests_house_edit1', 'submit_period_edit1'] # 对所有数值型的特征都计算skew，即计算一下偏度 skewed_feats = temp_ds[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False) print(\u0026quot;\\nSkew in numerical features: \\n\u0026quot;) skewness = pd.DataFrame({'Skew' :skewed_feats}) skewness.head()  Skew in numerical features:   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    Skew     watch_time_edit1 16.240   interests_house_edit1 15.395   submit_period_edit1 6.197   size_house_edit1 2.478     变换处理 skewness = skewness[abs(skewness) \u0026gt; 0.75] # 关于临界值，如何定，不知？？ print(\u0026quot;总共有 {} 数值型的特征做变换\u0026quot;.format(skewness.shape[0])) from scipy.special import boxcox1p skewed_features = skewness.index lam = 0.15 for feat in skewed_features: #all_data[feat] += 1 temp_ds[feat] = boxcox1p(temp_ds[feat], lam)  总共有 4 数值型的特征做变换  temp_ds.info()  \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; Int64Index: 474294 entries, 0 to 474300 Data columns (total 21 columns): community_house 474294 non-null int64 unit_house 474294 non-null int64 size_house_edit1 474294 non-null float64 size_house_edit1_addcata 474294 non-null int64 watch_time_edit1 474294 non-null float64 watch_time_edit1_addcata 474294 non-null int64 interests_house_edit1 474294 non-null float64 interests_house_edit1_addcata 474294 non-null int64 submit_period_edit1 474294 non-null float64 submit_period_edit1_addcata 474294 non-null int64 years_period_edit1 474294 non-null int64 tax_free_edit1 474294 non-null int64 smeter_price_edit1 474294 non-null float64 direction_edit1 474294 non-null int64 decoration_edit1 474294 non-null int64 elevator_edit1 474294 non-null int64 type_house_edit1 474294 non-null int64 years_house_type_edit1 474294 non-null int64 years_house_year_edit1 474294 non-null int64 region 474294 non-null int64 years_house_year_edit2 474294 non-null float64 dtypes: float64(6), int64(15) memory usage: 79.6 MB  这里偏差修正的问题，如果先进行labelcoding，那么如何保证label之后的结果是string？还是说提取单独的数值型列？ 最后，我决定单独取出\ntemp_ds.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    community_house unit_house size_house_edit1 size_house_edit1_addcata watch_time_edit1 watch_time_edit1_addcata interests_house_edit1 interests_house_edit1_addcata submit_period_edit1 submit_period_edit1_addcata years_period_edit1 tax_free_edit1 smeter_price_edit1 direction_edit1 decoration_edit1 elevator_edit1 type_house_edit1 years_house_type_edit1 years_house_year_edit1 region years_house_year_edit2     0 54124 71 6.616 0 -6.667 1 -6.667 1 -6.667 1 0 0 10.146 15 0 2 0 0 0 1 0.000   1 54121 71 8.169 0 -6.667 1 -6.667 1 -6.667 1 0 0 9.923 15 0 1 0 0 0 1 0.000   2 54117 71 6.885 0 -6.667 1 -6.667 1 -6.667 1 0 0 10.107 15 1 0 0 0 0 1 0.000   3 23640 71 6.395 0 -6.667 1 -6.667 1 -6.667 1 0 0 10.001 15 0 2 0 0 0 1 0.000   4 54131 71 7.165 0 -6.667 1 -6.667 1 -6.667 1 0 0 10.056 15 1 0 0 0 0 1 0.000     temp_ds.info()  \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; Int64Index: 474294 entries, 0 to 474300 Data columns (total 21 columns): community_house 474294 non-null int64 unit_house 474294 non-null int64 size_house_edit1 474294 non-null float64 size_house_edit1_addcata 474294 non-null int64 watch_time_edit1 474294 non-null float64 watch_time_edit1_addcata 474294 non-null int64 interests_house_edit1 474294 non-null float64 interests_house_edit1_addcata 474294 non-null int64 submit_period_edit1 474294 non-null float64 submit_period_edit1_addcata 474294 non-null int64 years_period_edit1 474294 non-null int64 tax_free_edit1 474294 non-null int64 smeter_price_edit1 474294 non-null float64 direction_edit1 474294 non-null int64 decoration_edit1 474294 non-null int64 elevator_edit1 474294 non-null int64 type_house_edit1 474294 non-null int64 years_house_type_edit1 474294 non-null int64 years_house_year_edit1 474294 non-null int64 region 474294 non-null int64 years_house_year_edit2 474294 non-null float64 dtypes: float64(6), int64(15) memory usage: 79.6 MB  哑变量处理 temp1 =dataset_used1.dtypes temp2 = temp1[temp1=='int64'].index for name in temp2: temp_ds[name] = temp_ds[name].astype(str) temp2_2 = temp1[temp1=='float64'].index for name in temp2_2: temp_ds[name] = temp_ds[name].astype(float)  temp_ds_use1 = temp_ds.drop(['community_house','years_house_year_edit1'], axis=1)  all_usedata = pd.get_dummies(temp_ds_use1)  all_usedata.shape  (474294, 243)  useful_dataset = all_usedata.sample(frac=0.1, random_state=123) all_usedata = None del all_usedata,temp_ds,temp_ds_use1,dataset_used1,dataset  建立模型 from sklearn.linear_model import ElasticNet, Lasso, BayesianRidge, LassoLarsIC from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor from sklearn.kernel_ridge import KernelRidge from sklearn.pipeline import make_pipeline from sklearn.preprocessing import RobustScaler from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone from sklearn.model_selection import KFold, cross_val_score, train_test_split from sklearn.metrics import mean_squared_error import xgboost as xgb import lightgbm as lgb  数据准备 X = useful_dataset.drop('smeter_price_edit1', axis=1) y = useful_dataset['smeter_price_edit1'] # 注意这一步！！数据结果与类型转换 X = X.as_matrix().astype(np.float) y = y.as_matrix().astype(np.float)  # all_data_na = (Train.isnull().sum() / len(Train)) * 100 # all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30] # missing_data = pd.DataFrame({'Missing Ratio' :all_data_na}) # f, ax = plt.subplots(figsize=(8, 8)) # plt.xticks(rotation='90') # sns.barplot(x=missing_data.index, y=all_data_na) # plt.xlabel('Features', fontsize=15) # plt.ylabel('Percent of missing values', fontsize=15) # plt.title('Percent missing data by feature', fontsize=15)  # 训练集与测试集划分 train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.2)  # 交叉验证函数 n_folds = 5 def rmsle_cv(model): kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train_x) rmse= np.sqrt(-cross_val_score(model, train_x, train_y, scoring=\u0026quot;neg_mean_squared_error\u0026quot;, cv = kf)) return(rmse)  模型函数 模型函数设定 lasso模型 lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.5, random_state=1))  ENet模型 ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.5, l1_ratio=.9, random_state=3))  KRR模型 KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)  GBoost模型 GBoost = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.1, max_depth=6, max_features='sqrt', min_samples_leaf=15, min_samples_split=10, loss='huber', random_state =5)  xgboost模型 model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, learning_rate=0.1, max_depth=6, min_child_weight=1.7817, n_estimators=1000, reg_alpha=0.4640, reg_lambda=0.8571, subsample=0.5213, silent=1, seed =7, nthread = -1)  LightGBM模型 model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5, learning_rate=0.05, n_estimators=720, max_bin = 55, bagging_fraction = 0.8, bagging_freq = 5, feature_fraction = 0.2319, feature_fraction_seed=9, bagging_seed=9, min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)  模型得分 score = rmsle_cv(lasso) print(\u0026quot;\\nLasso 得分: {:.4f} ({:.4f})\\n\u0026quot;.format(score.mean(), score.std()))  Lasso 得分: 0.6099 (0.0034)  score = rmsle_cv(ENet) print(\u0026quot;\\ENet 得分: {:.4f} ({:.4f})\\n\u0026quot;.format(score.mean(), score.std()))  \\ENet 得分: 0.6085 (0.0034)  lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.001, random_state=1)) score = rmsle_cv(lasso) print(\u0026quot;\\nLasso 得分: {:.4f} ({:.4f})\\n\u0026quot;.format(score.mean(), score.std()))  Lasso 得分: 0.3598 (0.0028)  ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.001, l1_ratio=.9, random_state=3)) score = rmsle_cv(ENet) print(\u0026quot;\\ENet 得分: {:.4f} ({:.4f})\\n\u0026quot;.format(score.mean(), score.std()))  \\ENet 得分: 0.3593 (0.0029)  score = rmsle_cv(KRR) print(\u0026quot;Kernel Ridge 得分: {:.4f} ({:.4f})\\n\u0026quot;.format(score.mean(), score.std()))  krr运行时间太长，没有执行和给出结果\nscore = rmsle_cv(GBoost) print(\u0026quot;Gradient Boosting 得分: {:.4f} ({:.4f})\\n\u0026quot;.format(score.mean(), score.std()))  Gradient Boosting 得分: 0.3328 (0.0041)  score = rmsle_cv(GBoost) print(\u0026quot;Gradient Boosting 得分: {:.4f} ({:.4f})\\n\u0026quot;.format(score.mean(), score.std()))  Gradient Boosting 得分: 0.3271 (0.0023)  score = rmsle_cv(model_xgb) print(\u0026quot;Xgboost 得分: {:.4f} ({:.4f})\\n\u0026quot;.format(score.mean(), score.std()))  Xgboost 得分: 0.3362 (0.0018)  score = rmsle_cv(model_xgb) print(\u0026quot;Xgboost 得分: {:.4f} ({:.4f})\\n\u0026quot;.format(score.mean(), score.std()))  Xgboost 得分: 0.3294 (0.0014)  score = rmsle_cv(model_lgb) print(\u0026quot;Xgboost 得分: {:.4f} ({:.4f})\\n\u0026quot;.format(score.mean(), score.std()))  Xgboost 得分: 0.3400 (0.0034)  模型融合 从简单的基本模型方法开始，考虑通过多模型融合的方式，尝试提高整体得分。首先构建简单类，扩充sklearn模型，然后根据初步融合结果进一步优化基模型参数，最后有所应用。\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone  基模型融合 class AverageModels(BaseEstimator, RegressorMixin, TransformerMixin): def __init__(self, models): self.models = models # 遍历所有模型 def fit(self, X, y): self.models_ = [clone(x) for x in self.models] for model in self.models_: model.fit(X, y) return self # 预估，并对预估结果做average def predict(self, X): predictions = np.column_stack([model.predict(X) for model in self.models_]) return np.mean(predictions, axis=1)  average_models = AverageModels(models=(lasso,ENet,GBoost))  score = rmsle_cv(average_models) print('对基模型进行集成之后的得分：{:.4f} ({:.4f})\\n'.format(score.mean(), score.std()))  对基模型进行集成之后的得分：0.3403 (0.0029)  对上面的结果对比，发现方差相比较单独模型有所降低，均值很低。故认为模型的融合起到改善预测结果的效果，所以考虑对单独模型构建更深层的模型融合。\nimport numpy as np # np.array((1,2,3),(11,22,33)) a=[1,2,3];b=[11,22,33];np.column_stack((a,b)) a=[[1,2,3],[10,20,30]];b=[[11,22,33],[110,220,330]];np.column_stack((a,b))  array([[ 1, 2, 3, 11, 22, 33], [ 10, 20, 30, 110, 220, 330]])  ab = np.column_stack([np.array(a),np.array(b)]) print(ab,ab.shape) ab.mean(axis=1)  [[ 1 11] [ 2 22] [ 3 33]] (3, 2) array([ 6., 12., 18.])  ab = np.column_stack([np.array(a)]) print(ab,ab.shape) ab.mean(axis=1)  [[1] [2] [3]] (3, 1) array([ 1., 2., 3.])  a=[1,2,3];b=[11,22,33]; np.column_stack((a,b)).mean(axis=1)  array([ 6., 12., 18.])  [np.column_stack([i]*4) for i in list(range(1,4))]  [array([[1, 1, 1, 1]]), array([[2, 2, 2, 2]]), array([[3, 3, 3, 3]])]  [np.column_stack([i]*4).mean(axis=1) for i in list(range(1,4))]  [array([ 1.]), array([ 2.]), array([ 3.])]  构建stacking averagd models的类 通过stacking的方式进行模型融合\nclass StackingAverageModels(BaseEstimator, RegressorMixin, TransformerMixin): def __init__(self, base_models, meta_model, n_folds): self.base_models = base_models self.meta_model = meta_model self.n_folds = n_folds # 遍历拟合原始模型 def fit(self, X, y): self.base_models_ = [list() for x in self.base_models] self.meta_model_ = clone(self.meta_model) kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=123) # 得到基模型之后，对out_of_fold的数据做预估，并为学习stacking的第二层做数据准备 out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)), dtype=np.float64) for i, model in enumerate(self.base_models): for train_index, holdout_index in kfold.split(X, y): instance = clone(model) self.base_models_[i].append(instance) instance.fit(X[train_index], y[train_index]) y_pred = instance.predict(X[holdout_index]) out_of_fold_predictions[holdout_index, i] = y_pred # 学习stacking模型 self.meta_model_.fit(out_of_fold_predictions, y) return self # 做stacking预估 def predict(self, X): meta_features = np.column_stack([ np.column_stack([model.predict(X) for model in base_models]).mean(axis=1) for base_models in self.base_models_]) return self.meta_model_.predict(meta_features)   测试meta-model的stacking结果  import datetime now = datetime.datetime.now() print(now)  2018-08-13 09:25:11.523184  stacked_averaged_model = StackingAverageModels(base_models=(ENet, GBoost), meta_model=lasso, n_folds=5) score = rmsle_cv(stacked_averaged_model) print('对基模型进行集成之后的得分：{:.4f} ({:.4f})\\n'.format(score.mean(), score.std()))  对基模型进行集成之后的得分：0.6017 (0.0024)  测试模型融合 def rmsle(y, y_pred): return np.sqrt(mean_squared_error(y, y_pred))  stacking stacked_averaged_model.fit(train_x, train_y)  stacked_train_pred = stacked_averaged_model.predict(train_x) stacked_pred = np.expm1(stacked_averaged_model.predict(test_x)) print(rmsle(train_y, stacked_train_pred)) print(rmsle(test_y, stacked_pred))  0.6017301501182424 30684.936189730954  xgboost model_xgb.fit(train_x, train_y) xgb_train_pred = model_xgb.predict(train_x) xgb_pred = np.expm1(model_xgb.predict(test_x)) print(rmsle(train_y, xgb_train_pred)) print(rmsle(test_y, xgb_pred))  0.28768534370101895 36986.13921042293  lightgbm model_lgb.fit(train_x, train_y) gbm_train_pred = model_lgb.predict(train_x) gbm_pred = np.expm1(model_lgb.predict(test_x)) print(rmsle(train_y, gbm_train_pred)) print(rmsle(test_y, gbm_pred))  0.33701688354734044 36068.90124078451  now = datetime.datetime.now() print(now)  np.expm1(0)  0.0  在测试集上面的表现很差，不理解为何差异非常大。\n结果 ensemble = stacked_pred*0.4 + xgb_pred*0.3 + lgb_pred*0.3   ","date":1538313964,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538313964,"objectID":"df9ac7687575b3c0e5afcbfdfa6749a4","permalink":"https://growing-bison.github.io/post/%E5%85%B3%E4%BA%8E%E9%93%BE%E5%AE%B6%E5%85%A8%E7%BD%91%E6%88%BF%E4%BB%B7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%8C%96%E6%8E%98%E9%A1%B9%E7%9B%AE/","publishdate":"2018-09-30T21:26:04+08:00","relpermalink":"/post/%E5%85%B3%E4%BA%8E%E9%93%BE%E5%AE%B6%E5%85%A8%E7%BD%91%E6%88%BF%E4%BB%B7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%8C%96%E6%8E%98%E9%A1%B9%E7%9B%AE/","section":"post","summary":"数据说明 1. 数据信息： - 数据量：40多万条观测，20多个列变量 - 时间：2018年5月前 2. 数据来源 - 作者：田昕峣 - 获取方式：https://gi","tags":["python","特征工程","sklearn"],"title":"关于链家全网房价数据分析挖掘项目","type":"post"},{"authors":null,"categories":[],"content":"","date":1537258231,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537258231,"objectID":"fa9c8893fd9eea2821494d6744a61fe7","permalink":"https://growing-bison.github.io/my/","publishdate":"2018-09-18T16:10:31+08:00","relpermalink":"/my/","section":"","summary":"","tags":[],"title":"My","type":"page"},{"authors":null,"categories":["data science","可视化"],"content":"这是关于如何对决策树分类器结果可视化的指导。 决策树结果的可视化-python 一、对于sklearn中的tree模型可视化 1、图形结果输出pdf保存本地；\n2、图形结果显示在界面\n1、图形结果输出pdf保存本地 from sklearn.datasets import load_iris import graphviz from sklearn import tree iris = load_iris() clf = tree.DecisionTreeClassifier() clf = clf.fit(iris.data, iris.target) dot_data = tree.export_graphviz(clf, out_file=None) graph = graphviz.Source(dot_data) graph.render(\u0026quot;保存的名字\u0026quot;)  2、图形结果显示在界面 from sklearn.datasets import load_iris import graphviz from sklearn import tree iris = load_iris() clf = tree.DecisionTreeClassifier() clf = clf.fit(iris.data, iris.target) # 首先生成graphvis所需的dot数据 dot_data = tree.export_graphviz(clf, # 已经训练的模型 out_file=None, # dot文件输出名称，默认tree.dot feature_names=iris.feature_names, # 特征变量名称 class_names=iris.target_names, # 目标变量名称 filled=False, # 颜色是否填充，默认False-不填充 rounded=True, # 字体设置，默认False special_characters=False # postgript对特殊字符处理，默认false ) # 将dot数据通过graphviz渲染 graph = graphviz.Source(dot_data) # 界面上直接显示 graph # 将图表保存本地为pdf（默认即为pdf）。？？如何保存为png graph.render('new2')  'new2.pdf'  二、对于xgboost的训练结果。 1、tree图形结果输出pdf保存本地；\n2、tree图形结果显示在界面；\n3、特征的重要性可视化。\ntree结果可以采用两种方式，①xgboost包中的plot_tree函数、②借助xgboost的to_graghviz和graphviz包\n特征重要新可视化，两种方式①xgboost包中的plot_importance函数、②自定义函数\n1、tree图形结果可视化①  from xgboost import plot_tree\nplot_tree(my_module) my_module # 已经fit\n 1、tree图形结果可视化② from xgboost import to_graphviz import graphviz # my_module # 已经fit dot_data = to_graphviz(my_module) graph = graphviz.Source(dot_data) # 显示在桌面 graph # 保存到本地 graph.render('new2')  2、特征重要新可视化①  from xgboost import plot_importance\nplot_importance(booster=my_module, importance_type=\u0026lsquo;weight\u0026rsquo;) # my_module # 已经fit\n 2、特征重要新可视化② def modelfit(alg, dtrain, predictors, performCV=True, printFeatureImportance=True, cv_folds=5): ''' 该函数方法借鉴tune parameter...大神的，改写而来 alg: 模型 dtrain: pd.DataFrame,包含训练特征和目标特征 predictors: list，包含训练特征的名称 performCV: 按默认 printFeatureImportance: 按默认 ''' #Fit the algorithm on the data alg.fit(dtrain[predictors], dtrain['Survived']) #Predict training set: dtrain_predictions = alg.predict(dtrain[predictors]) dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1] #Perform cross-validation: if performCV: cv_score = cross_validation.cross_val_score(alg, dtrain[predictors], dtrain['Survived'], cv=cv_folds, scoring='roc_auc') #Print model report: print (\u0026quot;\\nModel Report\u0026quot;) print (\u0026quot;Accuracy : %.4g\u0026quot; % metrics.accuracy_score(dtrain['Survived'].values, dtrain_predictions)) print (\u0026quot;AUC Score (Train): %f\u0026quot; % metrics.roc_auc_score(dtrain['Survived'], dtrain_predprob)) if performCV: print (\u0026quot;CV Score : Mean - %.7g | Std - %.7g | Min - %.7g | Max - %.7g\u0026quot; % (np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score))) #Print Feature Importance: if printFeatureImportance: feat_imp = pd.Series(alg.feature_importances_, predictors).sort_values(ascending=False) feat_imp.plot(kind='bar', title='Feature Importances') plt.ylabel('Feature Importance Score')  参考:\n1. How to Visualize Gradient Boosting Decision Trees With XGBoost in Python https://machinelearningmastery.com/visualize-gradient-boosting-decision-trees-xgboost-python/\n2. graphviz官方 https://pypi.org/project/graphviz/\n3. XGBoost Plotting API以及GBDT组合特征实践 https://blog.csdn.net/sb19931201/article/details/65445514\n4. sklearn官方 http://scikit-learn.org/stable/modules/tree.html#tree\n","date":1526120219,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526120219,"objectID":"607cd171a045d99cebfd4d147ee2e946","permalink":"https://growing-bison.github.io/post/%E5%86%B3%E7%AD%96%E6%A0%91%E5%8F%AF%E8%A7%86%E5%8C%96/","publishdate":"2018-05-12T18:16:59+08:00","relpermalink":"/post/%E5%86%B3%E7%AD%96%E6%A0%91%E5%8F%AF%E8%A7%86%E5%8C%96/","section":"post","summary":"这是关于如何对决策树分类器结果可视化的指导。 ","tags":["python","机器学习","可视化"],"title":"决策树结果可视化01","type":"post"},{"authors":null,"categories":["data science","statistic"],"content":"作为和师弟师妹简单介绍的文章，主要是有关线性回归建模与图形绘制 ================\n首先加载R的包 library(flexdashboard) library(rJava) library(car)  ## Loading required package: carData  library(plotly)  ## Loading required package: ggplot2 ## ## Attaching package: 'plotly' ## The following object is masked from 'package:ggplot2': ## ## last_plot ## The following object is masked from 'package:stats': ## ## filter ## The following object is masked from 'package:graphics': ## ## layout  library(ggplot2)  1. 散点图绘制 首先进行基本数据的观察。这里主要几个函数 - 设置文件路径： \u0026gt; setwd()\n 读入csv文件 \u0026gt; read.csv\n ggplot2函数 \u0026gt; ggplot()\n  setwd(\u0026quot;C:/Users/Administrator/Desktop/for_404-figure/datasource\u0026quot;) location1_data = read.csv('location1_ndvi1.csv',encoding='UTF-8') location1_lm = lm(location1_data$dry_matter.g.m2~location1_data$log_ndvi) ggplot(location1_data, aes(x=location1_data$log_ndvi, y=location1_data$dry_matter.g.m2., label=rownames(location1_data[1])))+ geom_point(size=2,color=\u0026quot;blue\u0026quot;)+ geom_text(hlabel=rownames(location1_data[1]), alpha=0.3,hjust=0,vjust=2,size=2)+ geom_rug()+ xlab('log_ndvi')+ ylab('dry matter:g/m2')  summary(location1_lm)  ## ## Call: ## lm(formula = location1_data$dry_matter.g.m2 ~ location1_data$log_ndvi) ## ## Residuals: ## Min 1Q Median 3Q Max ## -73.25 -31.98 -10.31 23.62 212.98 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 316.34 37.07 8.534 3.30e-12 *** ## location1_data$log_ndvi 257.71 43.78 5.887 1.51e-07 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 51.45 on 65 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.3478, Adjusted R-squared: 0.3377 ## F-statistic: 34.66 on 1 and 65 DF, p-value: 1.512e-07  2. 第一次 线性拟合结果中判断异常点 对第一次拟合模型不满意，希望探寻原数据中是否有异常值影响。 主要通过学生化残差——离群点、库克距离——强影响点、帽子值——高杠杆点\n 综合绘图函数:influencePlot() # car包  #car::influencePlot(location1_lm, #id.method=list(\u0026quot;x\u0026quot;,\u0026quot;y\u0026quot;),labels=rownames(location1_data[9]),id.n=nrow(location1_data[9])) car::influencePlot(location1_lm, method=\u0026quot;identify\u0026quot;)  ## StudRes Hat CookD ## 20 2.996216 0.14830962 0.69619267 ## 45 4.941031 0.04515861 0.42443172 ## 47 0.950764 0.12910081 0.06709942  3. 二次拟合：删除异常点后拟合 剔除部分异常点，重新拟合。\n anova对比：根据P值是否显著，判断是否有差别。 \u0026gt; anova()\n 赤池信息准则:选择小值 \u0026gt; AIC()\n  location1_data_edit20_3 = location1_data[-c(1,2,3,20,27,28,29,30,31,32,34,33,42,43,44,45,46),] location1_lm2 = lm(location1_data_edit20_3$dry_matter.g.m2.~location1_data_edit20_3$log_ndvi) summary(location1_lm2)  ## ## Call: ## lm(formula = location1_data_edit20_3$dry_matter.g.m2. ~ location1_data_edit20_3$log_ndvi) ## ## Residuals: ## Min 1Q Median 3Q Max ## -65.462 -21.262 -3.482 25.634 107.046 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 221.96 35.88 6.186 1.3e-07 *** ## location1_data_edit20_3$log_ndvi 156.39 41.68 3.753 0.000472 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 33.77 on 48 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.2268, Adjusted R-squared: 0.2107 ## F-statistic: 14.08 on 1 and 48 DF, p-value: 0.0004719  car::influencePlot(location1_lm2, method=\u0026quot;identify\u0026quot;)  ## StudRes Hat CookD ## 1 1.8470285 0.07352792 0.12889873 ## 23 3.5837724 0.02433724 0.12848349 ## 28 0.4995480 0.13561951 0.01988773 ## 30 0.5140136 0.23810427 0.04192757 ## 32 -2.0203538 0.02004473 0.03922777  # print('———————————————————————————————————————————————————————————————————————————') # 对比两个模型效果——ANOVA print(anova(location1_lm, location1_lm2))  ## Analysis of Variance Table ## ## Response: location1_data$dry_matter.g.m2 ## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## location1_data$log_ndvi 1 91732 91732 34.659 1.512e-07 *** ## Residuals 65 172036 2647 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  # print('———————————————————————————————————————————————————————————————————————————') # 对比两个模型效果——AIC print(AIC(location1_lm, location1_lm2))  ## df AIC ## location1_lm 3 722.1392 ## location1_lm2 3 497.7956  4. 二次拟合结果：对比实测值 通过图形观察拟合效果，对比观察与预测值，绘制对比图。\n 构建dataframe的函数 \u0026gt; data.frame()\n 绘图函数 \u0026gt; plot() text() point()\n 颜色获取函数 \u0026gt; hsv() hsv(h = 1, s = 1, v = 1, alpha=0.5)\n  颜色选取用hsv函数，对应hsv颜色空间参考https://codebeautify.org/hsv-to-rgb-converter\nhttp://sape.inf.usi.ch/quick-reference/ggplot2/colour\nr color cheatsheet\nfitdata_location1=data.frame(location1_data_edit20_3$dry_matter.g.m2., location1_data_edit20_3$log_ndvi) fit_location1=predict(location1_lm2, fitdata_location1, type = \u0026quot;response\u0026quot;) fitdata_location1_end=data.frame(fitdata_location1, fit_location1) plot(fitdata_location1_end$location1_data_edit20_3.log_ndvi, fitdata_location1_end$location1_data_edit20_3.dry_matter.g.m2., xlab=\u0026quot;ndvi\u0026quot;, ylab = \u0026quot;dry matter:g/m2\u0026quot;, main = \u0026quot;location1_ndvi与产量拟合_\u0026quot;) text1_color = hsv(h = 1, s = 0, v = 0.7, alpha=0.5) # 黑色 text(x=fitdata_location1_end$location1_data_edit20_3.log_ndvi, y=fitdata_location1_end$location1_data_edit20_3.dry_matter.g.m2., lab=rownames(location1_data[1]), adj=c(0,-0.5), pch=0.5, col = text1_color, cex = 0.6, # 字体大小设置 pos=2 # 位置 ) points(fitdata_location1_end$location1_data_edit20_3.log_ndvi, fitdata_location1_end$fit_location1, pch=10, col=2 # 对应黑色 ) text2_color = hsv(h = 1, s = 1, v = 1, alpha=0.3) # 红色 text(x=fitdata_location1_end$location1_data_edit20_3.log_ndvi, y=fitdata_location1_end$fit_location1, lab=rownames(location1_data[1]), adj=c(0,-0.5), pch=0.5, col=text2_color, cex = 0.6, pos =3, offset = 0.5 # 相对偏置距离 ) legend(\u0026quot;topleft\u0026quot;,c(\u0026quot;实际\u0026quot;,\u0026quot;拟合\u0026quot;), pch = c(1,10), col = c(1,2), title = \u0026quot;location1_ndvi与产量拟合\u0026quot;)  summary(location1_lm2)  ## ## Call: ## lm(formula = location1_data_edit20_3$dry_matter.g.m2. ~ location1_data_edit20_3$log_ndvi) ## ## Residuals: ## Min 1Q Median 3Q Max ## -65.462 -21.262 -3.482 25.634 107.046 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 221.96 35.88 6.186 1.3e-07 *** ## location1_data_edit20_3$log_ndvi 156.39 41.68 3.753 0.000472 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 33.77 on 48 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.2268, Adjusted R-squared: 0.2107 ## F-statistic: 14.08 on 1 and 48 DF, p-value: 0.0004719  5. 多个曲线对比 对多个月份的数据，绘制拟合曲线图，并在一张图中呈现。 - 求对数：log() - 合并两个表格（dataframe）:melt() - 绘图函数：qplot() - x轴的刻度间隔设置：scale_x_continuous()\nlibrary(ggplot2) library(data.table) #对比之前几个月的数据，即拟合结果需要保证前后一致性。06月、07月、08月，以及在相应位置的变化应该是呈现一种上升的关系 # test_x = rnorm(100,mean = 0, sd=1) # 生成满足正态分布的随机数 test_x = log(runif(100,min = 0,max = 1)) test_x = as.data.frame(test_x) a_06month = 80 b_06month = 220 a_07month = 55 b_07month = 185 a_o8month_morepoint = 160 b_o8month_morepoint = 210 a_o8month_lesspoint = 270 b_o8month_lesspoint = 320 df_data = data.frame(x=exp(test_x), y_06=test_x[,1]*a_06month+b_06month, y_07=test_x[,1]*a_07month+b_07month, y_08_more=test_x[,1]*a_o8month_morepoint+b_o8month_morepoint, y_08_less=test_x[,1]*a_o8month_lesspoint+b_o8month_lesspoint) df_melt_data = melt(df_data, id=1:5, measure=2:5) p = qplot(data = df_melt_data, x=df_melt_data$test_x, y=df_melt_data$value,geom='line', color=df_melt_data$variable, main = '拟合结果', xlab = 'NDVI value', ylab = '模拟的产量：g/m2') plot_end = p + scale_x_continuous(breaks = seq(0,1,0.1),labels = seq(0,1,0.1)) + scale_color_hue('图例') print(plot_end)  ","date":1525881600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525881600,"objectID":"b595fba69dff001c311522e0531bd74c","permalink":"https://growing-bison.github.io/post/r%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/","publishdate":"2018-05-10T00:00:00+08:00","relpermalink":"/post/r%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/","section":"post","summary":"作为和师弟师妹简单介绍的文章，主要是有关线性回归建模与图形绘制 ","tags":["R","线性回归","绘图"],"title":"数据处理-简单模型与绘图-01","type":"post"},{"authors":null,"categories":["编程","python"],"content":"这是关于如何建立python的自定义包的简单笔记，供简单参考。 关于python自定义包  使用的环境说明\n# !/usr/bin/env python3.5 # operation system: windows7   一、模块基本结构与引用 　常见的一个模块结构如下，一般还需要有一个test.py文件与package文件同目录，来测试这个包。 另外，__int__.py文件是必须要有，关键！它的存在表示此目录应该被作为一个package（包）。 在需要引用时，进入包的顶层目录rstools，导入某个函数。 package-rstools module-imagestretch 方法/函数-get_no_data\nfrom rstools.imagestretch import get_no_data get_no_data(file) # 当以*导入，package内的module受__int__.py限制。  二、package内部互相调用 package1/ __init__.py subPack1/ __init__.py module_11.py module_12.py module_13.py subPack2/ __init__.py module_21.py module_22.py   1、调用同一个package中的module，可直接import。如module_12.py希望调用module_11 {python} import module_11  2、调用非同一个package中的module，如module_11.py希望导入module_21.py中的FuncA {python} from subPack2.module_21 import module_11   三、__int__.py文件 该文件决定了包的导入和使用方式，需要很好设计。\n 1: all属性 {python} __int__=['dataprepare','datapansharpen']  2:  四、setput.py文件 文件中几个主要参数 - name：str，包名称 - version：str，版本号码 - url：包的链接，通常为 Github 上的链接，或者是 readthedocs 链接 - packages：需要包含的子包列表，find_packages()助查找 - setup_requires：指定依赖项 - test_suite：测试时运行的工具 - long_description：在一个来源处做说明介绍\nlong_description=open('README.md').read()   1：打包某个模块\n# 打包项目中某一个模块 from distutils.core import setup from setuptools import find_packages setup( name='production', version='v1.0', packages=find_packages(exclude=['production.moving_windows']) )  2：打包整个项目\nfrom setuptools import setup, find_packages setup(name = 'production',version = '1.0', py_modules = ['production.production_whole'], author = 'lqkpython', author_email = 'luqikun@gagogroup.com', url = 'http://www.gagogroup.com', description = 'A simple calculation for grass production ', packages=find_packages(exclude=['*.*']))   五、将项目进行打包 　整个项目是production/module，现在希望对其进行打包，从而能够为他人分享安装.\n 1、准备好的文件   2、打包整个项目  st=\u0026gt;start: 安装好打包所需的模块，setuptools模块 op1=\u0026gt;operation: 在module下的setup.py文件准备 op2=\u0026gt;operation: 编译:'python setup.py build' io1=\u0026gt;inputoutput: 在setup.py相同目录下多出build目录 op3=\u0026gt;operation: 打包:'python setup.py sdist' io2=\u0026gt;inputoutput: 打包后会在setup.py同目录下多出一个disk目录，存放打好的包 io3=\u0026gt;inputoutput: 完成打包 op4=\u0026gt;operation: 部署安装:'python setup.py install' e=\u0026gt;end: 完成整个项目打包 st-\u0026gt;op1-\u0026gt;op2-\u0026gt;io1-\u0026gt;op3-\u0026gt;io2-\u0026gt;op4-\u0026gt;e  补充的内容： 1. 将代码组织成包,想用import语句从另一个包名没有硬编码过的包的中导入子模块。 https://www.kancloud.cn/kancloud/python3-cookbook/47306\n练习文件链接如下 modul\n参考文档链接  1: python中自定义包的导入和使用 http://blog.csdn.net/maryhuan/article/details/22048763 2: Python包的编写和使用 http://blog.csdn.net/destinyuan/article/details/51704935 3: python 创建自己的包 http://blog.csdn.net/dai_jing/article/details/46005729 4: python 打包与部署 http://www.cnblogs.com/perfei/p/5139543.html 5: 这篇文章教会你打包Python程序 6: Python编写和发布一个Python CLI工具 ","date":1518164219,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1518164219,"objectID":"9143a067b397d98c841d1200b44bd5ff","permalink":"https://growing-bison.github.io/post/%E5%85%B3%E4%BA%8Epython%E8%87%AA%E5%AE%9A%E4%B9%89%E5%8C%85/","publishdate":"2018-02-09T16:16:59+08:00","relpermalink":"/post/%E5%85%B3%E4%BA%8Epython%E8%87%AA%E5%AE%9A%E4%B9%89%E5%8C%85/","section":"post","summary":"这是关于如何建立python的自定义包的简单笔记，供简单参考。 ","tags":["python","自定义包","模式匹配"],"title":"关于python自定义包","type":"post"},{"authors":null,"categories":["data science"],"content":"这是关于气象数据从nc到csv处理结果，并展示的过程。\n\n一、大体背景介绍 由于直接观测数据难以覆盖全面，希望能补充部分数据源，现需对开放的再分析数据进行简单分析。 - 提取 需求目标：从某种再分析数据源当中提取全国气象自动监测站点的数据，获取相应站点日数据。 涉及到几个气象要素：日均气温、日降水、日相对湿度、日辐射、日风速、日最大气温、日最小气温等七个要素。\n 已有资源及数据情况：\n数据源：格式-nc文件，空间分辨率0.125°的网格，时间分辨率小于24小时，时间跨度10年，总文件大小60G左右\n 处理分析 将再分析数据与观测站的数据进行比较，采用均方根误差RMSE作为指标，以月均值为主要对象，区域范围包括中国。\n  二、数据提取、处理、 提取的前提是对数据源的了解，明确单位、精度、气象要素的正常值范围等。 准备的文件：全国主要观测站点的经纬度坐标，格式csv/txt等。\n大体的流程图如下：\n2.1 提取的代码块1 简单提取程序脚本如下图：\n2.2 异常值和缺失值的代码块1 异常与缺失处理：\n{python} # find missing value \u0026amp;\u0026amp; put NaN missing = df.ix[(df[df.columns[7]] == 32766)] #print('########temperature missing########') for index, row in missing.iterrows(): #print(row[0], row[4], row[5], row[6], row[7]) print(index) df[df == 32766] = np.nan # get temperature tmp = df[df.columns[7]] * 0.1  2.3 要素转换的代码块1 主要是对将辐射转成日照时长。\n{python} def dursun(lat_deg, dt): t = 24.0/pi*(acos(-tan(lat_rad)*tan(theta))) return t def Ra(): Ra = 24 * 60 * Gsc * dr * ( (ws * sin(lat_rad) * sin(delta)) + ( cos(lat_rad) * cos(delta) * sin(ws))) / pi return Ra  2.4 数据提取结果 考虑存在csv格式文件，文件数量=7个气象要素×194个站点，大小200多Mb。\n 三、数据分析 准备的数据：\n1. 约200个站点的再分析日值数据（即从nc文件中提取出来，并清洗、转换好）；\n2. 约200个站点的观测站日值数据。\n3. 全国观测站点的经纬度坐标信息。\n上述数据均保存在csv文件当中，统一保存的编码格式。\n比较累年月值（12个月），月值（某年某月），累年值（某年）等的再分析数据对比观测站数据的误差情况。\n数据组织与流向如下图：\n   源 要素（温度、湿度等） 时间分辨率 文件数量及大小     再分析 \u0026hellip; 10年日值 1300个左右，200-300M   观测站 \u0026hellip; 10年日值 194个，100M左右    3.1 文件与数据管理 数据组织变化涉及几个步骤，以其中一个要素分析为例说明如下：\n3.2 数据分析处理与输出 {python} CMA_datasource_dir = '/media/dira/' station_194_dir = '/media/latlon.csv' df = pd.read_csv(station_194_dir, header=0, sep=',') star_time = datetime(2004, 1, 1).strftime('%Y-%m-%d') end_time = datetime(2016, 7, 31).strftime('%Y-%m-%d') # 2、读入194个CMA观测数据 val_3 = os.popen(\u0026quot;find /media/dir2 -name '%s*'\u0026quot; \\ % (stid)).read() #年份、月份提取 obs_vlu_m = grouped_by_month.mean()  3.4 数据分析处理与输出 对应累积年值的计算结果,包含194个站点1-12个月的对应RSEM、相关系数等。\n四、数据观测与图表绘制 1. 观测多个站点在不同时间尺度上的误差分布，误差分布与海拔高度间关系。 2. 空间分布上不同站点的情况。 准备的数据：  某个要素的约200个站点的误差及相关系数数据\n 工具平台：R、GIS、python\n  4.1 简单频次分布图 对数据中的RSME绘制简单的分布图\n4.2 时间序列分布图 多个站点的月值图 4.3 空间分布图 在python上简单绘制的结果 借助GIS绘制的结果 4.4 简单结论 1.　温度日值、月值的准确性较高，整体上和观测值基本相符，在具体应用时需区分不同地区\n2.　准确性较低的区域原因： + 受复杂地形等因素所影响 + 另一方面可能是观测数据本身站点少、观测质量\n五、小案例线性回归 如何在已有数据基础之上，筛除异常值，并得到最终拟合曲线\n","date":1515485819,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515485819,"objectID":"993d6e6f37122997e552f993ef370a37","permalink":"https://growing-bison.github.io/post/%E6%B0%94%E8%B1%A1%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/","publishdate":"2018-01-09T16:16:59+08:00","relpermalink":"/post/%E6%B0%94%E8%B1%A1%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/","section":"post","summary":"这是关于气象数据从nc到csv处理结果，并展示的过程。\n\n","tags":["R","数据处理","绘图","气象数据"],"title":"数据处理之气象数据","type":"post"},{"authors":["Ding Luyu","Lu Qikun","Xie Lina","et al"],"categories":null,"content":"Further details on the publication can be seen in pdf file. ","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"7e6bf96dbfc571916d8cda0aecef232c","permalink":"https://growing-bison.github.io/publication/pubulication1/","publishdate":"2016-01-01T00:00:00Z","relpermalink":"/publication/pubulication1/","section":"publication","summary":"The open lots and manure stockpiles of dairy farm are major sources of greenhouse gas (GHG) emissions in typical dairy cow housing and manure management system in China. GHG (CO(2), CH(4) and N(2)O) emissions from the ground level of brick-paved open lots and uncovered manure stockpiles were estimated according to the field measurements of a typical dairy farm in Beijing by closed chambers in four consecutive seasons. Location variation and manure removal strategy impacts were assessed on GHG emissions from the open lots. Estimated CO(2), CH(4) and N(2)O emissions from the ground level of the open lots were 137.5±64.7 kg hd(-1) yr(-1), 0.45±0.21 kg hd(-1) yr(-1) and 0.13±0.08 kg hd(-1) yr(-1), respectively. There were remarkable location variations of GHG emissions from different zones (cubicle zone vs. aisle zone) of the open lot. However, the emissions from the whole open lot were less affected by the locations. After manure removal, lower CH(4) but higher N(2)O emitted from the open lot. Estimated CO(2), CH(4) and N(2)O emissions from stockpile with a stacking height of 55±12 cm were 858.9±375.8 kg hd(-1) yr(-1), 8.5±5.4 kg hd(-1) yr(-1) and 2.3±1.1 kg hd(-1) yr(-1), respectively. In situ storage duration, which estimated by manure volatile solid contents (VS), would affect GHG emissions from stockpiles. Much higher N(2)O was emitted from stockpiles in summer due to longer manure storage.","tags":null,"title":"Greenhouse gas emissions from dairy open lot and manure stockpile in northern China: A case study","type":"publication"},{"authors":["Ding Luyu","Lu Qikun","Wang Chaoyuan","et al"],"categories":null,"content":"Further details on the publication can be seen in pdf file. ","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"c755842b47de4b931cf3916c1bda9576","permalink":"https://growing-bison.github.io/publication/pubulication2/","publishdate":"2015-01-01T00:00:00Z","relpermalink":"/publication/pubulication2/","section":"publication","summary":"This study was aimed at evaluating the influence of chamber configuration (enclosure area and height), the use of a mixing fan and the wind speed induced by the fan at the emitting surface (head space mixing) on the accuracy for the closed chamber in gas emission measurement at low emission rates (","tags":null,"title":"Effects of configuration and headspace mixing on the accuracy of closed chambers for dairy farm gas emission measurement","type":"publication"}]